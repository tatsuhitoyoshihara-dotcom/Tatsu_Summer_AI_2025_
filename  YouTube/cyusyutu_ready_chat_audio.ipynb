{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMeqJ4fLpiBysbiDKmyWWUl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3NdPXDTiAtfG"},"outputs":[],"source":["# ============================================\n","# YouTube 10分チャンク出力（フィルタ保存版 + サンプル表示）\n","# - 対象: transcription_progress の status==\"done\"（= chat+audio揃い）の video_id（7件想定）\n","# - フィルタ: 各チャンクで chat_only の \"content\" 文字数合計 >= MIN_CHATONLY_CHARS（100）\n","# - 保存: 上記を満たすチャンクのみ chat-only / chat+audio の両方を保存\n","# - サンプル: 採用されたチャンクから最大 SAMPLE_TAKE 件抜き、両ファイルの先頭10行を表示\n","# ============================================\n","\n","from __future__ import annotations\n","import os, re, json, sys, subprocess, random\n","from typing import List, Dict, Optional, Iterable, Tuple\n","\n","# 依存パッケージ\n","def _ensure(p):\n","    try:\n","        __import__(p)\n","    except Exception:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", p])\n","\n","for pkg in (\"pandas\",\"gcsfs\",\"tqdm\"):\n","    _ensure(pkg)\n","\n","import pandas as pd\n","import gcsfs\n","from tqdm.auto import tqdm\n","\n","# -----------------------\n","# 設定\n","# -----------------------\n","BATCH_ID = \"20250910_104532\"\n","BUCKET   = \"dena-ai-intern-yoshihara-data\"\n","RUN_TAG  = \"9_17\"            # 出力パスに含めるタグ\n","\n","# 出力挙動\n","CHUNK_SECONDS        = 600    # 10分\n","MIN_CHATONLY_CHARS   = 100    # 採用判定（chat_onlyのcontent合計）\n","OVERWRITE            = False  # 既存ファイルを上書きする場合 True\n","\n","# 既知の done 7件（progress CSVが無い/壊れている場合のフォールバック）\n","KNOWN_DONE_IDS = ['KEU1fA03VyE','MKiq-UOgNwc','QVJAcQI0ihM',\n","                  'X-p9zwW_UFA','Zw4gWI1cnlk','q9h92Oyhs8Q','vQ3IiAKlRJs']\n","\n","# サンプル表示\n","SAMPLE_TAKE          = 10     # 採用チャンクから最大この件数を抜き出して確認\n","SAMPLE_MAX_LINES     = 10     # 各ファイルから表示する最大行数\n","\n","# -----------------------\n","# パス\n","# -----------------------\n","BASE_PREFIX   = f\"gs://{BUCKET}/Archive/{BATCH_ID}\"\n","CHAT_DIR      = f\"{BASE_PREFIX}/chat\"\n","TRANS_DIR     = f\"{BASE_PREFIX}/transcripts\"\n","PROGRESS_CSV  = f\"{TRANS_DIR}/transcription_progress_{BATCH_ID}.csv\"\n","\n","OUT_BASE      = f\"{BASE_PREFIX}/chunks/{RUN_TAG}\"\n","OUT_COMBINED  = f\"{OUT_BASE}/yt_comment_speech_combined\"\n","OUT_CHATONLY  = f\"{OUT_BASE}/yt_comment_only\"\n","OUT_EXT       = \".txt\"\n","\n","# -----------------------\n","# ユーティリティ\n","# -----------------------\n","def safe_exists(fs, path: str) -> bool:\n","    try:\n","        return fs.exists(path)\n","    except Exception:\n","        return False\n","\n","def to_int_seconds_safe(x) -> int:\n","    v = pd.to_numeric(x, errors=\"coerce\")\n","    v = 0 if pd.isna(v) else int(v)\n","    return max(0, v)\n","\n","def ensure_no_trailing_slash(prefix: str) -> str:\n","    return prefix[:-1] if prefix.endswith(\"/\") else prefix\n","\n","def output_path(prefix: str, video_id: str, chunk_idx: int, ext: str = OUT_EXT) -> str:\n","    prefix = ensure_no_trailing_slash(prefix)\n","    return f\"{prefix}/{video_id}_{chunk_idx}{ext}\"\n","\n","def iter_chunk_indices(max_t: int, chunk_seconds: int = CHUNK_SECONDS) -> Iterable[int]:\n","    if max_t < 0:\n","        return []\n","    last_chunk = (max_t // chunk_seconds) + 1\n","    return range(1, last_chunk + 1)\n","\n","def build_text_lines(df: pd.DataFrame) -> List[str]:\n","    \"\"\"[SOURCE] 本文 形式の行リスト（本文中の改行はスペースに）。\"\"\"\n","    if df is None or df.empty:\n","        return []\n","    lines = []\n","    for row in df.itertuples(index=False):\n","        text_clean = str(row.content).replace(\"\\n\", \" \").strip()\n","        if not text_clean:\n","            continue\n","        lines.append(f\"[{row.source}] {text_clean}\")\n","    return lines\n","\n","def chars_sum_of_contents(df: pd.DataFrame) -> int:\n","    \"\"\"行の 'content' のトータル文字数（改行除去）\"\"\"\n","    if df is None or df.empty:\n","        return 0\n","    return int(df[\"content\"].astype(str).str.replace(\"\\n\",\" \", regex=False).str.len().sum())\n","\n","def write_text_gcs(fs: gcsfs.GCSFileSystem, gcs_path: str, text: str, overwrite: bool = False) -> bool:\n","    if (not overwrite) and fs.exists(gcs_path):\n","        return False  # skipped\n","    with fs.open(gcs_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(text if text.endswith(\"\\n\") else text + \"\\n\")\n","    return True\n","\n","# -----------------------\n","# YouTube Chat パーサ\n","# -----------------------\n","def _extract_text_from_runs(message_obj: dict | None) -> str:\n","    if not message_obj:\n","        return \"\"\n","    runs = message_obj.get(\"runs\", [])\n","    parts = []\n","    for r in runs:\n","        if \"text\" in r:\n","            parts.append(r[\"text\"])\n","        elif \"emoji\" in r:\n","            emo = r[\"emoji\"]\n","            alt = emo.get(\"shortcuts\") or emo.get(\"searchTerms\") or []\n","            parts.append(alt[0] if alt else \"\")\n","    return \"\".join(parts)\n","\n","def read_youtube_chat_jsonl_to_df(fs: gcsfs.GCSFileSystem, chat_jsonl_gcs: str, video_id: str) -> pd.DataFrame:\n","    rows = []\n","    idx = 0\n","    if not safe_exists(fs, chat_jsonl_gcs):\n","        return pd.DataFrame(columns=[\"video_id\",\"source\",\"t\",\"content\",\"orig_idx\"])\n","    with fs.open(chat_jsonl_gcs, \"r\") as f:\n","        for line in f:\n","            line = line.strip()\n","            if not line:\n","                continue\n","            try:\n","                obj = json.loads(line)\n","            except Exception:\n","                continue\n","\n","            rc = obj.get(\"replayChatItemAction\", {})\n","            actions = rc.get(\"actions\", []) or []\n","\n","            # 経過ms（動画内オフセット）\n","            elapsed_ms = None\n","            if \"videoOffsetTimeMsec\" in rc:\n","                try:\n","                    elapsed_ms = int(rc[\"videoOffsetTimeMsec\"])\n","                except Exception:\n","                    elapsed_ms = None\n","            if elapsed_ms is None:\n","                continue\n","\n","            t = to_int_seconds_safe(elapsed_ms // 1000)\n","\n","            for act in actions:\n","                add = act.get(\"addChatItemAction\")\n","                if not add:\n","                    continue\n","                item = add.get(\"item\", {})\n","                msg = item.get(\"liveChatTextMessageRenderer\")\n","                if not msg:\n","                    continue\n","                text = _extract_text_from_runs(msg.get(\"message\"))\n","                if not text.strip():\n","                    continue\n","                rows.append({\n","                    \"video_id\": video_id,\n","                    \"source\": \"CHAT\",\n","                    \"t\": t,\n","                    \"content\": text.strip(),\n","                    \"orig_idx\": idx\n","                })\n","                idx += 1\n","\n","    df = pd.DataFrame(rows)\n","    if not df.empty:\n","        df = df.sort_values([\"t\",\"orig_idx\"], kind=\"mergesort\").reset_index(drop=True)\n","    else:\n","        df = pd.DataFrame(columns=[\"video_id\",\"source\",\"t\",\"content\",\"orig_idx\"])\n","    return df\n","\n","# -----------------------\n","# Whisper transcript ローダ（JSON優先→SRT）\n","# -----------------------\n","def _parse_srt_timestamp(ts: str) -> float:\n","    h, m, rest = ts.split(\":\")\n","    s, ms = rest.split(\",\")\n","    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000.0\n","\n","def load_youtube_transcript_to_df(fs: gcsfs.GCSFileSystem, video_id: str) -> pd.DataFrame:\n","    json_path = f\"{TRANS_DIR}/{video_id}.json\"\n","    srt_path  = f\"{TRANS_DIR}/{video_id}.srt\"\n","    segs: List[Dict] = []\n","\n","    # JSON（segments）優先\n","    if safe_exists(fs, json_path):\n","        try:\n","            with fs.open(json_path, \"r\") as f:\n","                obj = json.load(f)\n","            for i, seg in enumerate(obj.get(\"segments\", []) or []):\n","                text = (seg.get(\"text\") or \"\").strip()\n","                if not text:\n","                    continue\n","                start = float(seg.get(\"start\", 0.0))\n","                t = to_int_seconds_safe(int(start))\n","                segs.append({\"video_id\": video_id, \"source\": \"SPEECH\", \"t\": t, \"content\": text, \"orig_idx\": i})\n","        except Exception:\n","            segs = []\n","\n","    # SRT フォールバック\n","    if (not segs) and safe_exists(fs, srt_path):\n","        try:\n","            with fs.open(srt_path, \"r\") as f:\n","                content = f.read()\n","            blocks = re.split(r\"\\n\\s*\\n\", content.strip(), flags=re.MULTILINE)\n","            idx = 0\n","            for b in blocks:\n","                lines = [ln.strip(\"\\r\") for ln in b.splitlines() if ln.strip()]\n","                if not lines:\n","                    continue\n","                if lines[0].isdigit():\n","                    lines = lines[1:]\n","                if not lines:\n","                    continue\n","                m = re.match(r\"(\\d{2}:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d{2}:\\d{2}:\\d{2},\\d{3})\", lines[0])\n","                if not m:\n","                    continue\n","                start_s = m.group(1)\n","                text = \"\\n\".join(lines[1:]).strip()\n","                if not text:\n","                    continue\n","                start = _parse_srt_timestamp(start_s)\n","                t = to_int_seconds_safe(int(start))\n","                segs.append({\"video_id\": video_id, \"source\": \"SPEECH\", \"t\": t, \"content\": text, \"orig_idx\": idx})\n","                idx += 1\n","        except Exception:\n","            segs = []\n","\n","    df = pd.DataFrame(segs)\n","    if not df.empty:\n","        df = df.sort_values([\"t\",\"orig_idx\"], kind=\"mergesort\").reset_index(drop=True)\n","    else:\n","        df = pd.DataFrame(columns=[\"video_id\",\"source\",\"t\",\"content\",\"orig_idx\"])\n","    return df\n","\n","# -----------------------\n","# 対象 video_id（done）を特定\n","# -----------------------\n","def find_done_video_ids(fs: gcsfs.GCSFileSystem) -> List[str]:\n","    if safe_exists(fs, PROGRESS_CSV):\n","        try:\n","            df = pd.read_csv(PROGRESS_CSV)\n","            if {\"video_id\",\"status\"} <= set(df.columns):\n","                vids = df.loc[df[\"status\"]==\"done\",\"video_id\"].astype(str).dropna().unique().tolist()\n","                if vids:\n","                    return vids\n","        except Exception:\n","            pass\n","    return KNOWN_DONE_IDS\n","\n","# -----------------------\n","# マージ & チャンク付与\n","# -----------------------\n","def merge_chat_speech(chat_df: pd.DataFrame, speech_df: pd.DataFrame) -> pd.DataFrame:\n","    if chat_df is None or chat_df.empty:\n","        chat_df = pd.DataFrame(columns=[\"video_id\",\"source\",\"t\",\"content\",\"orig_idx\"])\n","    if speech_df is None or speech_df.empty:\n","        speech_df = pd.DataFrame(columns=[\"video_id\",\"source\",\"t\",\"content\",\"orig_idx\"])\n","\n","    chat_df   = chat_df[chat_df[\"content\"].astype(str).str.len() > 0].copy()\n","    speech_df = speech_df[speech_df[\"content\"].astype(str).str.len() > 0].copy()\n","\n","    merged = pd.concat([chat_df, speech_df], ignore_index=True)\n","    if merged.empty:\n","        return pd.DataFrame(columns=[\"video_id\",\"source\",\"t\",\"content\",\"orig_idx\",\"chunk_idx\"])\n","\n","    source_order = merged[\"source\"].map({\"CHAT\":0, \"SPEECH\":1}).fillna(2).astype(int)\n","    merged = merged.assign(_source_order=source_order)\n","    merged = merged.sort_values([\"t\",\"_source_order\",\"orig_idx\"], kind=\"mergesort\").drop(columns=[\"_source_order\"])\n","    merged = merged.reset_index(drop=True)\n","    merged[\"chunk_idx\"] = (merged[\"t\"] // CHUNK_SECONDS) + 1\n","    return merged\n","\n","def add_chunk_idx(df: pd.DataFrame) -> pd.DataFrame:\n","    if df is None or df.empty:\n","        return pd.DataFrame(columns=[\"video_id\",\"source\",\"t\",\"content\",\"orig_idx\",\"chunk_idx\"])\n","    out = df.sort_values([\"t\",\"orig_idx\"], kind=\"mergesort\").reset_index(drop=True).copy()\n","    out[\"chunk_idx\"] = (out[\"t\"] // CHUNK_SECONDS) + 1\n","    return out\n","\n","# -----------------------\n","# メイン（保存 + サンプル表示）\n","# -----------------------\n","def main():\n","    fs = gcsfs.GCSFileSystem()\n","\n","    # done=7件を確定\n","    done_ids = find_done_video_ids(fs)\n","\n","    created, skipped_exist, skipped_filter, empty_chunks = 0, 0, 0, 0\n","    errors: List[Tuple[str,str]] = []\n","    accepted_records: List[Tuple[str,int,str,str,int]] = []  # (vid, chunk_idx, chat_path, comb_path, chat_chars)\n","\n","    print(f\"Output root: {OUT_BASE}\")\n","    print(f\"Targets ({len(done_ids)}): {done_ids}\")\n","\n","    pbar = tqdm(done_ids, desc=\"YouTube VODs (done only)\", unit=\"video\")\n","    for vid in pbar:\n","        try:\n","            pbar.set_postfix_str(f\"video_id={vid}\")\n","\n","            chat_path_file = f\"{CHAT_DIR}/{vid}.live_chat.json\"\n","            chat_df   = read_youtube_chat_jsonl_to_df(fs, chat_path_file, vid)\n","            speech_df = load_youtube_transcript_to_df(fs, vid)\n","\n","            merged = merge_chat_speech(chat_df, speech_df)\n","            chat_df = add_chunk_idx(chat_df)\n","\n","            max_t_candidates = []\n","            if not merged.empty:  max_t_candidates.append(int(merged[\"t\"].max()))\n","            if not chat_df.empty: max_t_candidates.append(int(chat_df[\"t\"].max()))\n","            max_t = max(max_t_candidates) if max_t_candidates else -1\n","            if max_t < 0:\n","                continue\n","\n","            live_chunks = list(iter_chunk_indices(max_t, CHUNK_SECONDS))\n","            inner = tqdm(live_chunks, desc=f\"write {vid}\", leave=False)\n","            for ci in inner:\n","                # 抽出\n","                part_c = chat_df[chat_df[\"chunk_idx\"] == ci] if not chat_df.empty else pd.DataFrame(columns=chat_df.columns)\n","                part_m = merged[merged[\"chunk_idx\"] == ci] if not merged.empty else pd.DataFrame(columns=merged.columns)\n","\n","                # 文字数（content合計）でフィルタ判定\n","                chat_chars = chars_sum_of_contents(part_c)\n","                if chat_chars < MIN_CHATONLY_CHARS:\n","                    skipped_filter += 1\n","                    inner.set_postfix_str(f\"filter< {MIN_CHATONLY_CHARS} chars (chat_only) at chunk {ci}\")\n","                    continue\n","\n","                # 行生成\n","                lines_c = build_text_lines(part_c.assign(source=\"CHAT\") if not part_c.empty else part_c)\n","                lines_m = build_text_lines(part_m)\n","\n","                if not lines_c:\n","                    empty_chunks += 1\n","                    inner.set_postfix_str(f\"empty chat_only lines at chunk {ci}\")\n","                    continue\n","                if not lines_m:\n","                    # 通常は起きないが安全側チェック\n","                    empty_chunks += 1\n","                    inner.set_postfix_str(f\"empty merged lines at chunk {ci}\")\n","                    continue\n","\n","                # 保存\n","                out_chat = output_path(OUT_CHATONLY, vid, ci, OUT_EXT)\n","                out_comb = output_path(OUT_COMBINED, vid, ci, OUT_EXT)\n","\n","                ok_c = write_text_gcs(fs, out_chat, \"\\n\".join(lines_c), overwrite=OVERWRITE)\n","                ok_m = write_text_gcs(fs, out_comb, \"\\n\".join(lines_m), overwrite=OVERWRITE)\n","\n","                created += int(ok_c) + int(ok_m)\n","                # 既存でスキップ（上書きOFFの場合）\n","                skipped_exist += int((not ok_c)) + int((not ok_m))\n","\n","                accepted_records.append((vid, ci, out_chat, out_comb, chat_chars))\n","                inner.set_postfix_str(f\"wrote chunk {ci}\")\n","            inner.close()\n","\n","        except Exception as e:\n","            errors.append((vid, str(e)))\n","            continue\n","\n","    pbar.close()\n","    print(\"\\n=== 完了レポート（done=7件 & フィルタ適用） ===\")\n","    print(f\"作成: {created} / 既存スキップ: {skipped_exist} / フィルタ不採用: {skipped_filter} / 空チャンク: {empty_chunks}\")\n","    if errors:\n","        print(f\"\\nエラー {len(errors)} 件（一部抜粋）:\")\n","        for vid, msg in errors[:20]:\n","            print(f\"  video_id={vid}: {msg}\")\n","\n","    # ---- 採用チャンクの概要 ----\n","    if not accepted_records:\n","        print(\"\\n[採用チャンクなし] フィルタ閾値を下げるか、対象を見直してください。\")\n","        return\n","\n","    # 動画ごとの採用数\n","    df_acc = pd.DataFrame(accepted_records, columns=[\"video_id\",\"chunk_idx\",\"chat_path\",\"combined_path\",\"chat_chars\"])\n","    agg = df_acc.groupby(\"video_id\").size().rename(\"accepted_chunks\").reset_index()\n","    print(\"\\n=== 採用チャンク件数（video_id別） ===\")\n","    print(agg.to_string(index=False))\n","\n","    # ---- サンプル表示（保存済みファイルを読み出し） ----\n","    # ソートした先頭から最大 SAMPLE_TAKE 件\n","    df_acc_sorted = df_acc.sort_values([\"video_id\",\"chunk_idx\"]).head(SAMPLE_TAKE)\n","\n","    print(f\"\\n=== サンプル表示: 採用チャンク 最大 {SAMPLE_TAKE} 件 / 各ファイル 先頭 {SAMPLE_MAX_LINES} 行 ===\")\n","    fs = gcsfs.GCSFileSystem()\n","    for row in df_acc_sorted.itertuples(index=False):\n","        vid, ci, chat_path, comb_path, chars_ = row\n","        print(\"\\n\" + \"=\"*120)\n","        print(f\"video_id={vid}  chunk={ci}  chat_chars={chars_}\")\n","        # chat_only\n","        print(f\"\\n[chat_only] {chat_path}\")\n","        try:\n","            with fs.open(chat_path, \"r\") as f:\n","                for i, line in enumerate(f):\n","                    if i >= SAMPLE_MAX_LINES:\n","                        print(\"... (truncated)\")\n","                        break\n","                    print(line.rstrip(\"\\n\"))\n","        except Exception as e:\n","            print(f\"(read error) {e}\")\n","\n","        # chat+audio\n","        print(f\"\\n[chat+audio] {comb_path}\")\n","        try:\n","            with fs.open(comb_path, \"r\") as f:\n","                for i, line in enumerate(f):\n","                    if i >= SAMPLE_MAX_LINES:\n","                        print(\"... (truncated)\")\n","                        break\n","                    print(line.rstrip(\"\\n\"))\n","        except Exception as e:\n","            print(f\"(read error) {e}\")\n","\n","    print(\"\\n--- 完了 ---\")\n","\n","# 実行\n","if __name__ == \"__main__\":\n","    main()\n"]}]}