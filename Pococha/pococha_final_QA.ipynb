{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQlVcL4PvKJhEW3yXwXuMi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_3TCHYv6qf6T"},"outputs":[],"source":["# --- New ipynb cell: Shuffle choices & GT with full logging (local+GCS), resumable execution ---\n","\n","from __future__ import annotations\n","import os, io, json, re, textwrap, datetime, hashlib\n","from typing import Dict, Any, List, Tuple, Optional\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import fsspec\n","import vertexai\n","from vertexai.generative_models import GenerativeModel, GenerationConfig\n","\n","# ====================== CONFIG ======================\n","PROJECT_ID        = \"dena-ai-intern-ds-dev-gcp\"\n","LOCATION          = \"us-central1\"\n","\n","# ---- Input: existing run (READ-ONLY) ----\n","RUN_TAG           = \"run_1050\"\n","RUNLOG_PREFIX     = f\"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_run_logs/{RUN_TAG}\"\n","ADOPT_LOG_PATH    = f\"{RUNLOG_PREFIX}/adopted_chunks_top100.csv\"\n","\n","# ---- Target lives ----\n","NUM_LIVE          = 40\n","LIVE_IDS_EXPLICIT: List[int] = []  # 空なら最初のNUM_LIVE件を自動選択\n","\n","# ---- Shuffle & model ----\n","RNG_SEED_BASE     = 20240917  # 安定シャッフルのベースシード（アイテム毎に派生させる）\n","MODEL_NAME        = \"gemini-2.5-pro\"\n","TEMPERATURE       = 0\n","\n","# ---- Output (LOCAL + GCS). 既存GCS成果物は一切更新しない。新しい専用プレフィックスを使う。----\n","GCS_BASE_PREFIX   = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_shuffle_eval\"\n","\n","# 再開用のラベル。空なら新規作成: 例) run_1050_10lives_YYYYmmdd_HHMMSS\n","RESUME_RUN_LABEL  = \"run_1050_40lives_20250917_061555\"     # 既存の run ラベル（例: \"run_1050_10lives_20250916_094017\"）を指定で再開\n","RUN_LABEL         = \"\"     # 上記が空のとき自動生成\n","\n","# 保存物（全問、全ステージで保存）\n","SAVE_EVERY_QA_SAMPLE = True  # 正解/不正解をすべて samples に保存\n","\n","# ====================== Helpers (FS/Vertex/JSON) ======================\n","def _dedent(s: str) -> str:\n","    return textwrap.dedent(s).strip()\n","\n","def fs_gcs():\n","    return fsspec.filesystem(\"gcs\")\n","\n","def init_vertex_ai(model_name: str) -> GenerativeModel:\n","    vertexai.init(project=PROJECT_ID, location=LOCATION)\n","    return GenerativeModel(model_name)\n","\n","def read_gcs_text(path: str) -> str:\n","    fs = fs_gcs()\n","    with fs.open(path, \"r\") as f:\n","        return f.read()\n","\n","def write_gcs_text(path: str, text: str) -> None:\n","    fs = fs_gcs()\n","    # gcsfsは'wb'書き換えのみ安定。appendは不可のことが多いので1ファイル=1レコードで保存する。\n","    with fs.open(path, \"w\") as f:\n","        f.write(text)\n","\n","def gcs_exists(path: str) -> bool:\n","    fs = fs_gcs()\n","    try:\n","        return fs.exists(path)\n","    except Exception:\n","        return False\n","\n","def gcs_glob(prefix: str, pattern: str=\"**\") -> List[str]:\n","    fs = fs_gcs()\n","    try:\n","        return sorted(fs.glob(f\"{prefix}/{pattern}\"))\n","    except Exception:\n","        return []\n","\n","def robust_json_loads(text: str) -> Optional[Dict[str, Any]]:\n","    # 1) 素直\n","    try:\n","        return json.loads(text)\n","    except Exception:\n","        pass\n","    # 2) ```json ... ``` 抜き出し\n","    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", text, flags=re.DOTALL)\n","    if m:\n","        try:\n","            return json.loads(m.group(1))\n","        except Exception:\n","            pass\n","    # 3) 先頭から最後の } まで\n","    m = re.search(r\"(\\{.*\\})\", text, flags=re.DOTALL)\n","    if m:\n","        s = m.group(1)\n","        s = s[: s.rfind(\"}\") + 1]\n","        try:\n","            return json.loads(s)\n","        except Exception:\n","            pass\n","    return None\n","\n","def call_model_return_both(model: GenerativeModel, content: str) -> Tuple[Dict[str, Any], str]:\n","    cfg = GenerationConfig(temperature=TEMPERATURE)\n","    raw = \"\"\n","    try:\n","        resp = model.generate_content([content], generation_config=cfg)\n","        try:\n","            raw = resp.text or \"\"\n","        except Exception:\n","            raw = \"\"\n","    except Exception:\n","        raw = \"\"\n","    obj = robust_json_loads(raw)\n","    if obj is None:\n","        obj = {\"raw_text\": raw}\n","    return obj, raw\n","\n","# ====================== Prompts ======================\n","def prompt_for_answering(qname: str, source_label: str) -> str:\n","    return _dedent(f\"\"\"\n","        次の『{source_label}だけ』を根拠に、与えられた1問({qname})の4択に回答してください。\n","        - 出力は JSON のみ（コードフェンス禁止）。\n","        - 回答は 0..3 の整数インデックス（choices 配列の添字）。\n","        - {{0から3までの数値}}の部分はLLMの回答の数値で置き換えて下さい\n","\n","        # 出力フォーマット\n","        {{\n","          \"answers\": {{\n","            \"{qname.lower()}_index\":  {{0から3までの数値を入れてください}}\n","          }}\n","        }}\n","\n","        # {qname}（question & choices のみ）\n","        # この後に {qname} のJSONを貼ります（answer_index/explanation は含みません）。\n","\n","        # 本文\n","    \"\"\")\n","\n","def build_content(qname: str, source_label: str, qjson: Dict[str, Any], body_text: Optional[str]=None) -> str:\n","    prompt = prompt_for_answering(qname, source_label)\n","    base = f\"\"\"{prompt}\n","\n","# {qname} JSON\n","{json.dumps(qjson, ensure_ascii=False, indent=2)}\n","\"\"\"\n","    if body_text and body_text.strip():\n","        return base + f\"\\n# 本文\\n{body_text}\"\n","    else:\n","        return base\n","\n","# ====================== Run directory layout (LOCAL + GCS) ======================\n","def make_run_label() -> str:\n","    now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    n = NUM_LIVE if not LIVE_IDS_EXPLICIT else len(LIVE_IDS_EXPLICIT)\n","    return f\"{RUN_TAG}_{n}lives_{now}\"\n","\n","def local_base_dir() -> str:\n","    return os.path.join(\"llmqa_shuffle_eval\", RUN_LABEL)\n","\n","def gcs_base_prefix() -> str:\n","    return f\"{GCS_BASE_PREFIX}/{RUN_LABEL}\"\n","\n","def ensure_local_dir(path: str) -> None:\n","    os.makedirs(path, exist_ok=True)\n","\n","def write_local_text(path: str, text: str) -> None:\n","    ensure_local_dir(os.path.dirname(path))\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(text)\n","\n","def write_local_json(path: str, obj: Any) -> None:\n","    write_local_text(path, json.dumps(obj, ensure_ascii=False, indent=2))\n","\n","def write_both_text(rel_path: str, text: str) -> None:\n","    # Local\n","    lpath = os.path.join(local_base_dir(), rel_path)\n","    write_local_text(lpath, text)\n","    # GCS\n","    gpath = f\"{gcs_base_prefix()}/{rel_path}\"\n","    try:\n","        write_gcs_text(gpath, text)\n","    except Exception:\n","        pass\n","\n","def write_both_json(rel_path: str, obj: Any) -> None:\n","    write_both_text(rel_path, json.dumps(obj, ensure_ascii=False, indent=2))\n","\n","def exists_local(rel_path: str) -> bool:\n","    return os.path.exists(os.path.join(local_base_dir(), rel_path))\n","\n","def exists_gcs(rel_path: str) -> bool:\n","    return gcs_exists(f\"{gcs_base_prefix()}/{rel_path}\")\n","\n","def exists_any(rel_path: str) -> bool:\n","    return exists_local(rel_path) or exists_gcs(rel_path)\n","\n","def read_local_json(rel_path: str) -> Optional[Any]:\n","    path = os.path.join(local_base_dir(), rel_path)\n","    try:\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            return json.load(f)\n","    except Exception:\n","        return None\n","\n","def read_gcs_json(rel_path: str) -> Optional[Any]:\n","    path = f\"{gcs_base_prefix()}/{rel_path}\"\n","    try:\n","        return json.loads(read_gcs_text(path))\n","    except Exception:\n","        return None\n","\n","def read_any_json(rel_path: str) -> Optional[Any]:\n","    obj = read_local_json(rel_path)\n","    if obj is not None:\n","        return obj\n","    return read_gcs_json(rel_path)\n","\n","def list_stage_log_files_local(stage_code: str) -> List[str]:\n","    base = os.path.join(local_base_dir(), f\"logs/{stage_code}/by_key\")\n","    if not os.path.isdir(base):\n","        return []\n","    return sorted([os.path.join(base, fn) for fn in os.listdir(base) if fn.endswith(\".json\")])\n","\n","def list_stage_log_files_gcs(stage_code: str) -> List[str]:\n","    return gcs_glob(f\"{gcs_base_prefix()}/logs/{stage_code}/by_key\", pattern=\"*.json\")\n","\n","def load_stage_logs(stage_code: str) -> pd.DataFrame:\n","    # Prefer local, fall back to GCS, union with de-dup by \"key\"\n","    records = []\n","    # Local\n","    for p in list_stage_log_files_local(stage_code):\n","        try:\n","            with open(p, \"r\", encoding=\"utf-8\") as f:\n","                records.append(json.load(f))\n","        except Exception:\n","            pass\n","    # GCS (include only keys not already loaded)\n","    seen = set(r.get(\"key\") for r in records if \"key\" in r)\n","    gpaths = list_stage_log_files_gcs(stage_code)\n","    fs = fs_gcs()\n","    for gp in gpaths:\n","        key = os.path.basename(gp).replace(\".json\",\"\")\n","        if key in seen:\n","            continue\n","        try:\n","            with fs.open(gp, \"r\") as f:\n","                records.append(json.load(f))\n","                seen.add(key)\n","        except Exception:\n","            pass\n","    if not records:\n","        return pd.DataFrame(columns=[\n","            \"key\",\"stage\",\"live_id\",\"chunk_idx\",\"qname\",\n","            \"new_gt_index\",\"pred_index\",\"correct\",\"invalid_format\"\n","        ])\n","    return pd.DataFrame(records)\n","\n","# ====================== Input loading & task build ======================\n","def load_runlog_df() -> pd.DataFrame:\n","    print(f\"RUN_TAG: {RUN_TAG}\")\n","    print(f\"Loading adopted log: {ADOPT_LOG_PATH}\")\n","    txt = read_gcs_text(ADOPT_LOG_PATH)\n","    return pd.read_csv(io.StringIO(txt))\n","\n","def select_live_ids(df: pd.DataFrame, k: int) -> List[int]:\n","    if LIVE_IDS_EXPLICIT:\n","        uniq = []\n","        s = set()\n","        for lid in LIVE_IDS_EXPLICIT:\n","            if lid not in s and lid in df.loc[df[\"status\"]==\"ok\",\"live_id\"].dropna().astype(int).values:\n","                s.add(lid); uniq.append(lid)\n","            if len(uniq)>=k: break\n","        return uniq\n","    cand = df.loc[df[\"status\"]==\"ok\",\"live_id\"].dropna().astype(int).tolist()\n","    seen, out = set(), []\n","    for lid in cand:\n","        if lid not in seen:\n","            seen.add(lid); out.append(lid)\n","        if len(out) >= k:\n","            break\n","    return out\n","\n","def _load_json_from_gcs_or_none(path: Optional[str]) -> Optional[Dict[str, Any]]:\n","    if not isinstance(path, str) or not path:\n","        return None\n","    try:\n","        return json.loads(read_gcs_text(path))\n","    except Exception:\n","        return None\n","\n","def build_tasks_from_runlog(df: pd.DataFrame, live_ids: List[int]) -> List[Dict[str, Any]]:\n","    tasks = []\n","    sub = df[(df[\"status\"]==\"ok\") & (df[\"live_id\"].isin(live_ids))].copy()\n","    sub = sub.dropna(subset=[\"mcq_full_path\"])\n","    for _, r in sub.iterrows():\n","        full = _load_json_from_gcs_or_none(r[\"mcq_full_path\"])\n","        pub  = _load_json_from_gcs_or_none(r.get(\"mcq_public_path\"))\n","        if not (isinstance(full, dict) and full.get(\"questions\") and len(full[\"questions\"])==2):\n","            continue\n","        if not (isinstance(pub, dict) and pub.get(\"questions\") and len(pub[\"questions\"])==2):\n","            pub = {\n","                \"questions\":[\n","                    {\"question\": full[\"questions\"][0][\"question\"], \"choices\": full[\"questions\"][0][\"choices\"][:4]},\n","                    {\"question\": full[\"questions\"][1][\"question\"], \"choices\": full[\"questions\"][1][\"choices\"][:4]},\n","                ]\n","            }\n","        q1_full, q2_full = full[\"questions\"][0], full[\"questions\"][1]\n","        q1_pub,  q2_pub  = pub[\"questions\"][0],  pub[\"questions\"][1]\n","        # texts\n","        combined_text = \"\"\n","        if isinstance(r.get(\"combined_path\"), str):\n","            try:\n","                combined_text = read_gcs_text(r[\"combined_path\"])\n","            except Exception:\n","                pass\n","        chat_text = \"\"\n","        if isinstance(r.get(\"chat_path\"), str):\n","            try:\n","                chat_text = read_gcs_text(r[\"chat_path\"])\n","            except Exception:\n","                pass\n","        has_chat = bool(isinstance(r.get(\"chat_path\"), str) and r[\"chat_path\"])\n","\n","        # Q1\n","        tasks.append({\n","            \"live_id\": int(r[\"live_id\"]),\n","            \"chunk_idx\": int(r[\"chunk_idx\"]) if pd.notna(r[\"chunk_idx\"]) else None,\n","            \"qname\": \"Q1\",\n","            \"question\": str(q1_pub[\"question\"]),\n","            \"choices\": list(q1_pub[\"choices\"])[:4],\n","            \"orig_gt_index\": int(q1_full[\"answer_index\"]),\n","            \"combined_text\": combined_text,\n","            \"chat_text\": chat_text,\n","            \"has_chat\": has_chat,\n","        })\n","        # Q2\n","        tasks.append({\n","            \"live_id\": int(r[\"live_id\"]),\n","            \"chunk_idx\": int(r[\"chunk_idx\"]) if pd.notna(r[\"chunk_idx\"]) else None,\n","            \"qname\": \"Q2\",\n","            \"question\": str(q2_pub[\"question\"]),\n","            \"choices\": list(q2_pub[\"choices\"])[:4],\n","            \"orig_gt_index\": int(q2_full[\"answer_index\"]),\n","            \"combined_text\": combined_text,\n","            \"chat_text\": chat_text,\n","            \"has_chat\": has_chat,\n","        })\n","    return tasks\n","\n","# ====================== Stable shuffle ======================\n","def _key_tuple_str(lid: int, chunk_idx: Optional[int], qname: str) -> str:\n","    return f\"{lid}_{'NA' if chunk_idx is None else chunk_idx}_{qname}\"\n","\n","def _seed_for_item(base_seed: int, lid: int, chunk_idx: Optional[int], qname: str) -> int:\n","    # 安定乱数用に blake2b から 64bit int を生成\n","    h = hashlib.blake2b(digest_size=8)\n","    h.update(f\"{base_seed}|{lid}|{chunk_idx}|{qname}\".encode(\"utf-8\"))\n","    return int.from_bytes(h.digest(), \"little\", signed=False)\n","\n","def stable_shuffle_choices_and_gt(choices: List[str], gt_index: int, base_seed: int,\n","                                  lid: int, chunk_idx: Optional[int], qname: str\n","                                 ) -> Tuple[List[str], int, List[int]]:\n","    seed = _seed_for_item(base_seed, lid, chunk_idx, qname)\n","    rng = np.random.default_rng(seed)\n","    perm = rng.permutation(4)\n","    new_choices = [choices[old] for old in perm]\n","    new_gt_index = int(np.where(perm == gt_index)[0][0])\n","    assert new_choices[new_gt_index] == choices[gt_index], \"Shuffle mismatch: correct not preserved\"\n","    return new_choices, new_gt_index, perm.tolist()\n","\n","def get_or_make_shuffle_record(t: Dict[str, Any]) -> Dict[str, Any]:\n","    \"\"\"\n","    1問（Q1/Q2）に対して、シャッフル結果を決定・保存（by_key JSON）。既存があればそれを使う。\n","    \"\"\"\n","    key = _key_tuple_str(t[\"live_id\"], t[\"chunk_idx\"], t[\"qname\"])\n","    rel = f\"shuffled/by_key/{key}.json\"\n","    rec = read_any_json(rel)\n","    if rec is not None:\n","        return rec\n","    new_choices, new_gt, perm = stable_shuffle_choices_and_gt(\n","        t[\"choices\"], t[\"orig_gt_index\"], RNG_SEED_BASE, t[\"live_id\"], t[\"chunk_idx\"], t[\"qname\"]\n","    )\n","    rec = {\n","        \"key\": key,\n","        \"live_id\": t[\"live_id\"],\n","        \"chunk_idx\": t[\"chunk_idx\"],\n","        \"qname\": t[\"qname\"],\n","        \"question\": t[\"question\"],\n","        \"orig_choices\": t[\"choices\"],\n","        \"new_choices\": new_choices,\n","        \"orig_gt_index\": t[\"orig_gt_index\"],\n","        \"new_gt_index\": new_gt,\n","        \"perm_new_to_old\": perm,  # new_idx -> old_idx\n","    }\n","    write_both_json(rel, rec)\n","    return rec\n","\n","# ====================== Prediction parsing ======================\n","def parse_pred_index(obj: Dict[str, Any], qname: str) -> Tuple[Optional[int], int]:\n","    if not isinstance(obj, dict):\n","        return (None, 1)\n","    ans = obj.get(\"answers\", {})\n","    if not isinstance(ans, dict):\n","        return (None, 1)\n","    key = f\"{qname.lower()}_index\"\n","    if key in ans:\n","        try:\n","            v = int(ans[key]);  return (v if 0<=v<=3 else None, 0 if 0<=v<=3 else 1)\n","        except Exception:\n","            return (None, 1)\n","    if \"index\" in ans:\n","        try:\n","            v = int(ans[\"index\"]);  return (v if 0<=v<=3 else None, 0 if 0<=v<=3 else 1)\n","        except Exception:\n","            return (None, 1)\n","    return (None, 1)\n","\n","# ====================== Stage execution (resumable, logs+samples saved) ======================\n","def stage_answer(model: GenerativeModel, tasks: List[Dict[str, Any]],\n","                 stage_code: str,\n","                 source_label: str, include_body: bool) -> List[Dict[str, Any]]:\n","    \"\"\"\n","    - 既存ログ(logs/{stage}/by_key/{key}.json)があればスキップ\n","    - 新規に処理した分は logs にJSON保存 + samples/{stage}/{correct|incorrect}/{key}.txt に保存\n","    - ローカルとGCSに同時保存\n","    \"\"\"\n","    new_logs = []\n","    pbar = tqdm(total=len(tasks), desc=f\"{stage_code}: 0/{len(tasks)}\", dynamic_ncols=True)\n","    done = 0\n","\n","    for t in tasks:\n","        key = _key_tuple_str(t[\"live_id\"], t[\"chunk_idx\"], t[\"qname\"])\n","        # 既にこのステージのログがあればスキップ\n","        log_rel = f\"logs/{stage_code}/by_key/{key}.json\"\n","        if exists_any(log_rel):\n","            done += 1\n","            pbar.set_description_str(f\"{stage_code}: {done}/{len(tasks)}\")\n","            pbar.update(1)\n","            continue\n","\n","        # シャッフル結果（存在しなければ作る）\n","        sh = get_or_make_shuffle_record(t)\n","        qjson = {\"question\": sh[\"question\"], \"choices\": sh[\"new_choices\"]}\n","        new_gt = int(sh[\"new_gt_index\"])\n","\n","        # 本文の用意\n","        body = \"\"\n","        if include_body:\n","            if \"combined\" in stage_code.lower():\n","                body = t.get(\"combined_text\",\"\") or \"\"\n","            elif \"chat\" in stage_code.lower():\n","                body = t.get(\"chat_text\",\"\") or \"\"\n","            else:\n","                body = \"\"\n","        # コンテンツ組み立て & 呼び出し\n","        content = build_content(t[\"qname\"], source_label, qjson, body_text=body)\n","        obj, raw = call_model_return_both(model, content)\n","        pred, invalid = parse_pred_index(obj, t[\"qname\"])\n","        correct = int(pred == new_gt) if (invalid == 0 and pred is not None) else None\n","\n","        # 1レコードのログ（このステージ・この問題の最終記録）\n","        log_rec = {\n","            \"key\": key,\n","            \"stage\": stage_code,\n","            \"live_id\": t[\"live_id\"],\n","            \"chunk_idx\": t[\"chunk_idx\"],\n","            \"qname\": t[\"qname\"],\n","            \"source_label\": source_label,\n","            \"include_body\": bool(include_body),\n","            \"question\": sh[\"question\"],\n","            \"choices\": sh[\"new_choices\"],\n","            \"orig_gt_index\": int(sh[\"orig_gt_index\"]),\n","            \"new_gt_index\": int(new_gt),\n","            \"pred_index\": (int(pred) if pred is not None else None),\n","            \"invalid_format\": int(invalid),\n","            \"correct\": (int(correct) if correct is not None else None),\n","            \"prompt_sent\": content,\n","            \"raw_response\": raw,\n","            \"ts\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n","        }\n","        # 保存（by_key JSON）\n","        write_both_json(log_rel, log_rec)\n","\n","        # samples にも（全件）\n","        if SAVE_EVERY_QA_SAMPLE:\n","            cls = \"correct\" if (correct == 1) else \"incorrect\" if (correct == 0) else \"invalid\"\n","            sample_rel = f\"samples/{stage_code}/{cls}/{key}.txt\"\n","            sample_txt = []\n","            sample_txt.append(f\"### META\\nstage: {stage_code}\\nkey: {key}\\nlive_id: {t['live_id']}\\nchunk_idx: {t['chunk_idx']}\\nqname: {t['qname']}\\n\")\n","            sample_txt.append(f\"orig_gt_index: {sh['orig_gt_index']}  new_gt_index: {new_gt}  pred_index: {pred}  correct: {correct}  invalid_format: {invalid}\")\n","            sample_txt.append(\"\\n### QUESTION (shuffled)\\n\" + json.dumps(qjson, ensure_ascii=False, indent=2))\n","            if include_body and body:\n","                sample_txt.append(\"\\n### BODY\\n\" + body)\n","            sample_txt.append(\"\\n### PROMPT SENT (exact)\\n\" + content)\n","            sample_txt.append(\"\\n### RAW RESPONSE\\n\" + raw)\n","            write_both_text(sample_rel, \"\\n\".join(sample_txt))\n","\n","        new_logs.append(log_rec)\n","\n","        done += 1\n","        pbar.set_description_str(f\"{stage_code}: {done}/{len(tasks)}\")\n","        pbar.update(1)\n","\n","    pbar.close()\n","    return new_logs\n","\n","# ====================== Metrics & distributions ======================\n","def index_distribution(series: pd.Series, k: int=4) -> Dict[int, float]:\n","    cnt = series.value_counts().reindex(range(k), fill_value=0).sort_index()\n","    tot = int(cnt.sum())\n","    if tot == 0:\n","        return {i: 0.0 for i in range(k)}\n","    share = (cnt / tot).fillna(0.0)\n","    return {int(i): float(share.iloc[i]) for i in range(k)}\n","\n","def summarize_stage_df(df: pd.DataFrame, qname: str, stage_code: str) -> Dict[str, Any]:\n","    sub = df[(df[\"qname\"]==qname) & (df[\"stage\"]==stage_code)].copy()\n","    provided = len(sub)\n","    valid = int((sub[\"invalid_format\"]==0).sum())\n","    correct_total = int(sub.loc[sub[\"invalid_format\"]==0, \"correct\"].fillna(0).astype(int).sum())\n","    acc = float(correct_total / valid) if valid else None\n","    pred_dist = index_distribution(sub.loc[sub[\"invalid_format\"]==0, \"pred_index\"].dropna().astype(int), k=4) if valid else {i:0.0 for i in range(4)}\n","    return {\n","        \"stage\": stage_code,\n","        \"qname\": qname,\n","        \"provided\": int(provided),\n","        \"answered_valid\": int(valid),\n","        \"correct_total\": int(correct_total),\n","        \"accuracy\": acc,\n","        \"pred_index_dist\": pred_dist,\n","        \"invalid_format_total\": int((sub[\"invalid_format\"]==1).sum()),\n","    }\n","\n","# ====================== Main flow ======================\n","# 0) Run label (new or resume)\n","if RESUME_RUN_LABEL.strip():\n","    RUN_LABEL = RESUME_RUN_LABEL.strip()\n","else:\n","    RUN_LABEL = RUN_LABEL or make_run_label()\n","\n","# Prepare base dirs\n","ensure_local_dir(local_base_dir())\n","\n","# Write manifest if not exists\n","manifest_rel = \"manifest.json\"\n","if not exists_any(manifest_rel):\n","    manifest = {\n","        \"run_label\": RUN_LABEL,\n","        \"run_tag\": RUN_TAG,\n","        \"num_live_requested\": NUM_LIVE,\n","        \"rng_seed_base\": RNG_SEED_BASE,\n","        \"model_name\": MODEL_NAME,\n","        \"temperature\": TEMPERATURE,\n","        \"created_at\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n","    }\n","    write_both_json(manifest_rel, manifest)\n","print(f\"RUN_LABEL: {RUN_LABEL}\")\n","print(f\"Local out: {local_base_dir()}\")\n","print(f\"GCS out:   {gcs_base_prefix()}\")\n","\n","# 1) Load runlog & select lives\n","run_df = load_runlog_df()\n","target_live_ids = select_live_ids(run_df, NUM_LIVE)\n","print(f\"選択 live_id: {target_live_ids}\")\n","\n","# Persist selected lives (for resume traceability)\n","selected_rel = \"selected_live_ids.json\"\n","if not exists_any(selected_rel):\n","    write_both_json(selected_rel, {\"live_ids\": target_live_ids})\n","\n","# 2) Build tasks\n","all_tasks = build_tasks_from_runlog(run_df, target_live_ids)\n","print(f\"総タスク数(Q1/Q2合計): {len(all_tasks)}\")\n","\n","# 3) Generate/Save shuffle records for all tasks (resumable)\n","print(\"シャッフル（安定）を決定・保存中...\")\n","for t in tqdm(all_tasks, total=len(all_tasks), desc=\"shuffle map\", dynamic_ncols=True):\n","    _ = get_or_make_shuffle_record(t)\n","\n","# 3-1) Show GT distributions before/after\n","def collect_gt_dists(tasks: List[Dict[str,Any]]) -> Tuple[Dict[int,float], Dict[int,float]]:\n","    q = [t for t in tasks]\n","    # load shuffled records to get new_gt_index\n","    orig, neww = [], []\n","    for t in q:\n","        sh = get_or_make_shuffle_record(t)\n","        orig.append(int(sh[\"orig_gt_index\"]))\n","        neww.append(int(sh[\"new_gt_index\"]))\n","    return index_distribution(pd.Series(orig)), index_distribution(pd.Series(neww))\n","\n","q1_tasks = [t for t in all_tasks if t[\"qname\"]==\"Q1\"]\n","q2_tasks = [t for t in all_tasks if t[\"qname\"]==\"Q2\"]\n","q1_orig_dist, q1_new_dist = collect_gt_dists(q1_tasks)\n","q2_orig_dist, q2_new_dist = collect_gt_dists(q2_tasks)\n","print(\"\\n=== GT index distribution (original vs shuffled) ===\")\n","print(\"Q1 original:\", q1_orig_dist)\n","print(\"Q1 shuffled:\", q1_new_dist)\n","print(\"Q2 original:\", q2_orig_dist)\n","print(\"Q2 shuffled:\", q2_new_dist)\n","\n","# 4) Init model (single model used across stages; names map to LLM2..LLM9 conceptually)\n","model = init_vertex_ai(MODEL_NAME)\n","\n","# 5) Run stages (resumable; each stage skips already-logged keys)\n","# Q1 primary: LLM2 (combined), LLM3 (nothing)\n","logs_q1_llm2_new = stage_answer(model, q1_tasks, stage_code=\"Q1_LLM2_combined\",\n","                                source_label=\"配信ログ本文（チャット＋音声書き起こし）\", include_body=True)\n","logs_q1_llm3_new = stage_answer(model, q1_tasks, stage_code=\"Q1_LLM3_nothing\",\n","                                source_label=\"問題文のみ\", include_body=False)\n","\n","# Load full logs (existing + new) to compute subset\n","df_q1_llm2 = load_stage_logs(\"Q1_LLM2_combined\")\n","df_q1_llm3 = load_stage_logs(\"Q1_LLM3_nothing\")\n","subset_q1 = df_q1_llm2.merge(df_q1_llm3, on=[\"key\",\"live_id\",\"chunk_idx\",\"qname\"], suffixes=(\"_llm2\",\"_llm3\"))\n","subset_q1 = subset_q1[(subset_q1[\"correct_llm2\"]==1) & ((subset_q1[\"correct_llm3\"].isna()) | (subset_q1[\"correct_llm3\"]==0))]\n","\n","subset_q1_keys = set(subset_q1[\"key\"].tolist())\n","subset_q1_tasks = [t for t in q1_tasks if _key_tuple_str(t[\"live_id\"], t[\"chunk_idx\"], t[\"qname\"]) in subset_q1_keys]\n","subset_q1_tasks_with_chat = [t for t in subset_q1_tasks if t[\"has_chat\"]]\n","\n","# Q1 extra: LLM4 (chat only), LLM5 (nothing)\n","logs_q1_llm4_new = stage_answer(model, subset_q1_tasks_with_chat, stage_code=\"Q1_LLM4_chat\",\n","                                source_label=\"チャット本文\", include_body=True)\n","logs_q1_llm5_new = stage_answer(model, subset_q1_tasks, stage_code=\"Q1_LLM5_nothing\",\n","                                source_label=\"問題文のみ\", include_body=False)\n","\n","# Q2 primary: LLM6 (combined), LLM7 (nothing)\n","logs_q2_llm6_new = stage_answer(model, q2_tasks, stage_code=\"Q2_LLM6_combined\",\n","                                source_label=\"配信ログ本文（チャット＋音声書き起こし）\", include_body=True)\n","logs_q2_llm7_new = stage_answer(model, q2_tasks, stage_code=\"Q2_LLM7_nothing\",\n","                                source_label=\"問題文のみ\", include_body=False)\n","\n","# Load full logs to compute subset for Q2\n","df_q2_llm6 = load_stage_logs(\"Q2_LLM6_combined\")\n","df_q2_llm7 = load_stage_logs(\"Q2_LLM7_nothing\")\n","subset_q2 = df_q2_llm6.merge(df_q2_llm7, on=[\"key\",\"live_id\",\"chunk_idx\",\"qname\"], suffixes=(\"_llm6\",\"_llm7\"))\n","subset_q2 = subset_q2[(subset_q2[\"correct_llm6\"]==1) & ((subset_q2[\"correct_llm7\"].isna()) | (subset_q2[\"correct_llm7\"]==0))]\n","\n","subset_q2_keys = set(subset_q2[\"key\"].tolist())\n","subset_q2_tasks = [t for t in q2_tasks if _key_tuple_str(t[\"live_id\"], t[\"chunk_idx\"], t[\"qname\"]) in subset_q2_keys]\n","subset_q2_tasks_with_chat = [t for t in subset_q2_tasks if t[\"has_chat\"]]\n","\n","# Q2 extra: LLM8 (chat only), LLM9 (nothing)\n","logs_q2_llm8_new = stage_answer(model, subset_q2_tasks_with_chat, stage_code=\"Q2_LLM8_chat\",\n","                                source_label=\"チャット本文\", include_body=True)\n","logs_q2_llm9_new = stage_answer(model, subset_q2_tasks, stage_code=\"Q2_LLM9_nothing\",\n","                                source_label=\"問題文のみ\", include_body=False)\n","\n","# 6) Collect all logs (existing+new) for metrics\n","stage_codes = [\"Q1_LLM2_combined\",\"Q1_LLM3_nothing\",\"Q1_LLM4_chat\",\"Q1_LLM5_nothing\",\n","               \"Q2_LLM6_combined\",\"Q2_LLM7_nothing\",\"Q2_LLM8_chat\",\"Q2_LLM9_nothing\"]\n","all_dfs = [load_stage_logs(st) for st in stage_codes]\n","logs_df = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n","\n","# 7) Summaries & distributions\n","summaries = {\"Q1\": {}, \"Q2\": {}}\n","for st in [\"Q1_LLM2_combined\",\"Q1_LLM3_nothing\",\"Q1_LLM4_chat\",\"Q1_LLM5_nothing\"]:\n","    summaries[\"Q1\"][st] = summarize_stage_df(logs_df, \"Q1\", st)\n","for st in [\"Q2_LLM6_combined\",\"Q2_LLM7_nothing\",\"Q2_LLM8_chat\",\"Q2_LLM9_nothing\"]:\n","    summaries[\"Q2\"][st] = summarize_stage_df(logs_df, \"Q2\", st)\n","\n","subset_sizes = {\n","    \"Q1_total\": len(subset_q1_tasks),\n","    \"Q1_with_chat\": len(subset_q1_tasks_with_chat),\n","    \"Q2_total\": len(subset_q2_tasks),\n","    \"Q2_with_chat\": len(subset_q2_tasks_with_chat),\n","}\n","\n","metrics = {\n","    \"run_label\": RUN_LABEL,\n","    \"run_tag\": RUN_TAG,\n","    \"num_live\": len(target_live_ids),\n","    \"gt_index_distribution\": {\n","        \"Q1_original\": q1_orig_dist,\n","        \"Q1_shuffled\": q1_new_dist,\n","        \"Q2_original\": q2_orig_dist,\n","        \"Q2_shuffled\": q2_new_dist,\n","    },\n","    \"stage_summaries\": summaries,\n","    \"subset_sizes\": subset_sizes,\n","    \"generated_at\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n","}\n","\n","# 8) Save metrics (local+GCS)\n","write_both_json(\"metrics.json\", metrics)\n","\n","# 9) Print human-readable summaries\n","def short_line(s: Dict[str, Any]) -> str:\n","    acc = \"NA\" if s[\"accuracy\"] is None else f\"{s['accuracy']:.3f}\"\n","    return (f\"{s['stage']}: provided={s['provided']} valid={s['answered_valid']} \"\n","            f\"correct={s['correct_total']} acc={acc} pred_dist={s['pred_index_dist']} invalid={s['invalid_format_total']}\")\n","\n","print(\"\\n=== Index distributions (GT before/after shuffle) ===\")\n","print(f\"Q1 original GT: {q1_orig_dist}\")\n","print(f\"Q1 shuffled GT: {q1_new_dist}\")\n","print(f\"Q2 original GT: {q2_orig_dist}\")\n","print(f\"Q2 shuffled GT: {q2_new_dist}\")\n","\n","print(\"\\n--- Q1 ---\")\n","for st in [\"Q1_LLM2_combined\",\"Q1_LLM3_nothing\",\"Q1_LLM4_chat\",\"Q1_LLM5_nothing\"]:\n","    print(short_line(summaries[\"Q1\"][st]))\n","\n","print(\"\\n--- Q2 ---\")\n","for st in [\"Q2_LLM6_combined\",\"Q2_LLM7_nothing\",\"Q2_LLM8_chat\",\"Q2_LLM9_nothing\"]:\n","    print(short_line(summaries[\"Q2\"][st]))\n","\n","print(\"\\n=== Conditional subset sizes ===\")\n","print(f\"Q1 subset (LLM2 correct & LLM3 incorrect): total={subset_sizes['Q1_total']}; with_chat={subset_sizes['Q1_with_chat']}\")\n","print(f\"Q2 subset (LLM6 correct & LLM7 incorrect): total={subset_sizes['Q2_total']}; with_chat={subset_sizes['Q2_with_chat']}\")\n","\n","print(f\"\\nAll done. Local outputs: {local_base_dir()}\")\n","print(f\"GCS outputs:          {gcs_base_prefix()}\")\n"]}]}