{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSpc42340Q6YA55jnzjXPW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0gS-FFUMqXmJ"},"outputs":[],"source":["# F_Combined_V_X (Rewritten to include combined (chat+audio) text in examples)\n","\n","# LLM1(gen) -> MCQ(Q1/Q2) from Combined\n","# LLM2(ans) -> Answer Q1 from Combined\n","# LLM4(ans) -> Answer same Q1 from Chat-only IF LLM2 correct & has_chat (conditional)\n","# LLM3(ans) -> Answer Q1 from Chat-only regardless (unconditional)\n","#\n","# LLM5(ans) -> Answer Q2 from Combined\n","# LLM7(ans) -> Answer same Q2 from Chat-only IF LLM5 correct & has_chat (conditional)\n","# LLM6(ans) -> Answer Q2 from Chat-only regardless (unconditional)\n","#\n","# Strict leakage control: answerers only see {question, choices}; no GT leaked\n","# Resume-safe: read existing artifacts; merge run logs; save to GCS + local\n","# Extra:\n","#   - Save EXACT API request texts for up to 2 calls per RUN_ID under run dir (local + GCS)\n","#   - Dump 3 correct and 3 incorrect examples (question, choices, chosen index) to files\n","#     ★ and include the 10分の chat+audio テキスト (combined_text) in each example object\n","#   - Count invalid-format answers (index not in {0,1,2,3} or unparsable) and report\n","#\n","# Requirements: google-cloud-aiplatform, gcsfs, fsspec, pandas, tqdm\n","\n","from __future__ import annotations\n","import os\n","import io\n","import re\n","import json\n","import textwrap\n","from typing import List, Dict, Any, Optional, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import fsspec\n","import vertexai\n","from vertexai.generative_models import GenerativeModel, GenerationConfig\n","\n","# ====================== CONFIG ======================\n","PROJECT_ID = \"dena-ai-intern-ds-dev-gcp\"\n","LOCATION   = \"us-central1\"\n","\n","# ---- Models (all gemini-2.5-pro as requested) ----\n","MODEL_NAME_QGEN             = \"gemini-2.5-pro\"  # LLM1 (MCQ generation) reading Combined\n","MODEL_NAME_Q1_COMBINED      = \"gemini-2.5-pro\"  # LLM2 (Q1 answer) reading Combined\n","MODEL_NAME_Q1_CHAT_UNCOND   = \"gemini-2.5-pro\"  # LLM3 (Q1 answer) reading Chat-only (unconditional)\n","MODEL_NAME_Q1_CHAT_COND     = \"gemini-2.5-pro\"  # LLM4 (Q1 answer) reading Chat-only (conditional)\n","MODEL_NAME_Q2_COMBINED      = \"gemini-2.5-pro\"  # LLM5 (Q2 answer) reading Combined\n","MODEL_NAME_Q2_CHAT_UNCOND   = \"gemini-2.5-pro\"  # LLM6 (Q2 answer) reading Chat-only (unconditional)\n","MODEL_NAME_Q2_CHAT_COND     = \"gemini-2.5-pro\"  # LLM7 (Q2 answer) reading Chat-only (conditional)\n","\n","GEN_TEMPERATURE = 0\n","\n","# Input index: local CSV listing comment files\n","SUFFIX_LIST_CSV = \"pococha_comment_sorted_suffixes.csv\"\n","TOP_N_LIVE = 100  # ← 実行対象のライブ数\n","\n","# Pre-generated 10-min chunk texts (GCS)\n","COMBINED_PREFIX = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_comment_speeech_combined\"\n","CHATONLY_PREFIX = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_comment_only\"\n","\n","# ====== Run isolation (★重要) ======\n","RUN_ID = 1050  # ← 適宜変更\n","print(f\"RUN_ID:{RUN_ID}, TOP_N_LIVE:{TOP_N_LIVE}\")\n","RUN_TAG = f\"run_{RUN_ID}\"\n","\n","# ---- Local mirror ----\n","LOCAL_OUT_BASE = os.path.join(\"llmqa_runs\", RUN_TAG)\n","LOCAL_QUESTIONS_FULL_DIR     = os.path.join(LOCAL_OUT_BASE, \"questions_full\")            # GT付き（LLM1の出力）\n","LOCAL_QUESTIONS_PUBLIC_DIR   = os.path.join(LOCAL_OUT_BASE, \"questions_public\")          # 公開用（question & choicesのみ）\n","LOCAL_ANS_Q1_COMBINED_DIR    = os.path.join(LOCAL_OUT_BASE, \"answers_q1_combined\")       # LLM2\n","LOCAL_ANS_Q1_CHAT_UNCOND_DIR = os.path.join(LOCAL_OUT_BASE, \"answers_q1_chat_uncond\")    # LLM3\n","LOCAL_ANS_Q1_CHAT_COND_DIR   = os.path.join(LOCAL_OUT_BASE, \"answers_q1_chat_cond\")      # LLM4 (conditional)\n","LOCAL_ANS_Q2_COMBINED_DIR    = os.path.join(LOCAL_OUT_BASE, \"answers_q2_combined\")       # LLM5\n","LOCAL_ANS_Q2_CHAT_UNCOND_DIR = os.path.join(LOCAL_OUT_BASE, \"answers_q2_chat_uncond\")    # LLM6\n","LOCAL_ANS_Q2_CHAT_COND_DIR   = os.path.join(LOCAL_OUT_BASE, \"answers_q2_chat_cond\")      # LLM7 (conditional)\n","LOCAL_RUNLOG_DIR             = os.path.join(LOCAL_OUT_BASE, \"run_logs\")\n","LOCAL_DEBUG_INPUTS_DIR       = os.path.join(LOCAL_OUT_BASE, \"debug_inputs\")\n","LOCAL_EXAMPLES_DIR           = os.path.join(LOCAL_OUT_BASE, \"examples\")\n","\n","# ---- Outputs (GCS, run-scoped) ----\n","QUESTIONS_PREFIX_BASE = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_mcq_questions\"\n","ANSWERS_PREFIX_BASE   = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_mcq_answers\"\n","RUNLOG_PREFIX_BASE    = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_run_logs\"\n","\n","QUESTIONS_PREFIX   = f\"{QUESTIONS_PREFIX_BASE}/{RUN_TAG}\"\n","ANSWERS_PREFIX     = f\"{ANSWERS_PREFIX_BASE}/{RUN_TAG}\"\n","RUNLOG_PREFIX      = f\"{RUNLOG_PREFIX_BASE}/{RUN_TAG}\"\n","ADOPT_LOG_PATH     = f\"{RUNLOG_PREFIX}/adopted_chunks_top{TOP_N_LIVE}.csv\"\n","METRICS_JSON_PATH  = f\"{RUNLOG_PREFIX}/metrics_top{TOP_N_LIVE}.json\"\n","DEBUG_INPUTS_PREFIX= f\"{RUNLOG_PREFIX}/debug_inputs\"\n","EXAMPLES_PREFIX    = f\"{RUNLOG_PREFIX}/examples\"\n","\n","# ---- GCS artifact paths ----\n","Q_FULL_OUT    = lambda lid, idx: f\"{QUESTIONS_PREFIX}/full/{lid}_{idx}.json\"\n","Q_PUB_OUT     = lambda lid, idx: f\"{QUESTIONS_PREFIX}/public/{lid}_{idx}.json\"\n","\n","A_Q1C_OUT     = lambda lid, idx: f\"{ANSWERS_PREFIX}/q1_combined/{lid}_{idx}.json\"       # LLM2\n","A_Q1CHATU_OUT = lambda lid, idx: f\"{ANSWERS_PREFIX}/q1_chat_uncond/{lid}_{idx}.json\"    # LLM3\n","A_Q1CHATC_OUT = lambda lid, idx: f\"{ANSWERS_PREFIX}/q1_chat_cond/{lid}_{idx}.json\"      # LLM4 (conditional)\n","\n","A_Q2C_OUT     = lambda lid, idx: f\"{ANSWERS_PREFIX}/q2_combined/{lid}_{idx}.json\"       # LLM5\n","A_Q2CHATU_OUT = lambda lid, idx: f\"{ANSWERS_PREFIX}/q2_chat_uncond/{lid}_{idx}.json\"    # LLM6\n","A_Q2CHATC_OUT = lambda lid, idx: f\"{ANSWERS_PREFIX}/q2_chat_cond/{lid}_{idx}.json\"      # LLM7 (conditional)\n","\n","# Filtering\n","MIN_COMBINED_CHARS = 500  # Combined が短すぎるチャンクはスキップ\n","CHECKPOINT_EVERY   = 50\n","\n","# Debug input saving (save exact payloads sent to API): up to 2 per RUN_ID\n","_MAX_SAVED_INPUTS = 2\n","_saved_input_count = 0\n","\n","# ====================== Vertex AI / FS Helpers ======================\n","def init_vertex_ai(model_name: str) -> GenerativeModel:\n","    vertexai.init(project=PROJECT_ID, location=LOCATION)\n","    return GenerativeModel(model_name)\n","\n","def fs_gcs() -> fsspec.AbstractFileSystem:\n","    return fsspec.filesystem(\"gcs\")\n","\n","def write_gcs_text(path: str, text: str) -> None:\n","    fs = fs_gcs()\n","    with fs.open(path, \"w\") as f:\n","        f.write(text)\n","\n","def read_gcs_text(path: str) -> str:\n","    fs = fs_gcs()\n","    with fs.open(path, \"r\") as f:\n","        return f.read()\n","\n","def gcs_glob(prefix: str, pattern: str) -> List[str]:\n","    fs = fs_gcs()\n","    return sorted(fs.glob(f\"{prefix}/{pattern}\"))\n","\n","def ensure_local_dirs() -> None:\n","    for d in [\n","        LOCAL_QUESTIONS_FULL_DIR,\n","        LOCAL_QUESTIONS_PUBLIC_DIR,\n","        LOCAL_ANS_Q1_COMBINED_DIR,\n","        LOCAL_ANS_Q1_CHAT_UNCOND_DIR,\n","        LOCAL_ANS_Q1_CHAT_COND_DIR,\n","        LOCAL_ANS_Q2_COMBINED_DIR,\n","        LOCAL_ANS_Q2_CHAT_UNCOND_DIR,\n","        LOCAL_ANS_Q2_CHAT_COND_DIR,\n","        LOCAL_RUNLOG_DIR,\n","        LOCAL_DEBUG_INPUTS_DIR,\n","        LOCAL_EXAMPLES_DIR,\n","    ]:\n","        os.makedirs(d, exist_ok=True)\n","\n","def _save_debug_input_if_needed(content: str, label: str):\n","    \"\"\"Save EXACT payload sent to the API (prompt+doc). Max 2 files per RUN_ID.\"\"\"\n","    global _saved_input_count\n","    if _saved_input_count >= _MAX_SAVED_INPUTS:\n","        return\n","    _saved_input_count += 1\n","    # Local\n","    local_path = os.path.join(LOCAL_DEBUG_INPUTS_DIR, f\"{_saved_input_count:02d}_{label}.txt\")\n","    with open(local_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(content)\n","    # GCS\n","    gcs_path = f\"{DEBUG_INPUTS_PREFIX}/{_saved_input_count:02d}_{label}.txt\"\n","    try:\n","        write_gcs_text(gcs_path, content)\n","    except Exception:\n","        pass\n","\n","# ====================== Model Call (robust JSON parsing) ======================\n","def call_gemini_json_with_content(model: GenerativeModel, content: str, save_label: Optional[str]=None) -> Dict[str, Any]:\n","    \"\"\"Call model with prebuilt content; robust to safety blocks & messy JSON; optionally save the EXACT payload.\"\"\"\n","    if save_label:\n","        _save_debug_input_if_needed(content, save_label)\n","\n","    cfg = GenerationConfig(temperature=GEN_TEMPERATURE)\n","    try:\n","        resp = model.generate_content([content], generation_config=cfg)\n","        try:\n","            raw = resp.text or \"\"\n","        except Exception:\n","            raw = \"\"\n","    except Exception:\n","        raw = \"\"\n","\n","    # direct JSON\n","    try:\n","        return json.loads(raw)\n","    except Exception:\n","        pass\n","    # ```json ... ``` extract\n","    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", raw, flags=re.DOTALL)\n","    if m:\n","        try:\n","            return json.loads(m.group(1))\n","        except Exception:\n","            pass\n","    # greedy { ... }\n","    m = re.search(r\"(\\{.*\\})\", raw, flags=re.DOTALL)\n","    if m:\n","        s = m.group(1)\n","        s = s[: s.rfind(\"}\") + 1]\n","        try:\n","            return json.loads(s)\n","        except Exception:\n","            pass\n","    return {\"raw_text\": raw}\n","\n","# ====================== Prompts (MCQ prompt is NOT changed) ======================\n","def _dedent(s: str) -> str:\n","    return textwrap.dedent(s).strip()\n","\n","def prompt_for_mcq_fixed() -> str:\n","    # ★「変更しない」指定のため、与えられたまま\n","    return _dedent(\"\"\"\n","        以下の『配信ログ本文』だけを根拠に、ライブ配信に関する4択問題を日本語で作成してください。外部知識の持ち込みは禁止です。\n","\n","        # 出題ルール（厳守）\n","        - 問題数: 2 問（ちょうど2問。増減しない）\n","        - 1問目は、次の固定文言をそのまま問題文に使う：\n","          『配信中に出てきた話題は、以下の四つの選択肢のうちどれが正しいですか？』\n","        - 2問目は、次のテンプレートの {TOPIC} を 1問目の正解選択肢のテキスト（短い名詞句）に置換して使う：\n","          『配信中、{TOPIC}に関して行われた会話の内容は、以下のどれか？』\n","          ※ 出力時に {TOPIC} を残さないこと。\n","        - 似た選択肢は作らない。正解は各問ちょうど1つ。\n","        - あいまい表現や主観的解釈は禁止。本文の根拠のみ。\n","        - 挨拶など普遍的な内容は題材にしない。\n","        - 出力は JSON のみ。説明文やコードフェンス（```）は禁止。\n","        - 以下に与えた具体的な例とは異なる問題と選択肢を作成する。\n","        - 配信者/リスナーの区別や経過時間を問題文に含めない。\n","        - 説明(explanation)はRecitation回避のため原文の直接引用禁止。根拠は短い要約（100文字以内）。\n","        - 配信ログ本文を読んだら、必ず正解が選べるような問題を作ってください。\n","\n","        # 出力フォーマット（厳守）\n","        {\n","          \"questions\": [\n","            {\n","              \"question\": \"配信中に出てきた話題は、以下の四つの選択肢のうちどれが正しいですか？\",\n","              \"choices\": [\"選択肢A\", \"選択肢B\", \"選択肢C\", \"選択肢D\"],\n","              \"answer_index\": 0,\n","              \"explanation\": \"本文からの根拠（直接引用）\"\n","            },\n","            {\n","              \"question\": \"配信中、{TOPIC}に関して行われた会話の内容は、以下のどれか？\",\n","              \"choices\": [\"選択肢A\", \"選択肢B\", \"選択肢C\", \"選択肢D\"],\n","              \"answer_index\": 0,\n","              \"explanation\": \"本文からの根拠\"\n","            }\n","          ]\n","        }\n","\n","        # 例（one-shot）\n","        一つ目の問い\n","        問題文：配信中に出てきた話題は、以下の四つの選択肢のうちどれが正しいですか？\n","        選択肢A：唐揚げ\n","        選択肢B：不動産投資\n","        選択肢C：大阪の万博\n","        選択肢D：オーストラリアでのマラソン\n","        正解のインデックス：0\n","\n","        二つ目の問い\n","        問題文：配信中、{TOPIC}に関して行われた会話の内容は、以下のどれか？\n","        選択肢A：唐揚げはもも肉より胸肉の方が良い\n","        選択肢B：唐揚げの値上げが嫌だ\n","        選択肢C：いつも唐揚げ食べると胃もたれする\n","        選択肢D：唐揚げにはレモンをかけるべきか否か\n","        正解のインデックス：3\n","\n","        # 配信ログ本文\n","    \"\"\")\n","\n","def _prompt_for_answering(qname: str, source_label: str) -> str:\n","    # qname: \"Q1\" or \"Q2\"; source_label: \"配信ログ本文（チャット＋音声書き起こし）\" or \"チャット本文\"\n","    return _dedent(f\"\"\"\n","        次の『{source_label}だけ』を根拠に、与えられた1問({qname})の4択に回答してください。\n","        - 出力は JSON のみ（コードフェンス禁止）。\n","        - 回答は 0..3 の整数インデックス（choices 配列の添字）。\n","\n","        # 出力フォーマット\n","        {{\n","          \"answers\": {{\n","            \"{qname.lower()}_index\": 0\n","          }}\n","        }}\n","\n","        # {qname}（question & choices のみ）\n","        # この後に {qname} のJSONを貼ります（answer_index/explanation は含みません）。\n","\n","        # 本文\n","    \"\"\")\n","\n","# ====================== Utilities: parsing & cleaning ======================\n","_TAG_AT_LINE_START = re.compile(r'^(?:\\[[^\\]]+\\]\\s*)+')\n","\n","def clean_plain_text(s: str) -> str:\n","    if not isinstance(s, str) or not s:\n","        return \"\"\n","    s = s.replace(\"\\r\\n\", \"\\n\")\n","    lines = []\n","    for line in s.split(\"\\n\"):\n","        line = _TAG_AT_LINE_START.sub(\"\", line).strip()\n","        lines.append(line)\n","    return \"\\n\".join(lines).strip()\n","\n","def parse_chunk_idx_from_path(path: str) -> Optional[int]:\n","    base = os.path.basename(path)\n","    try:\n","        return int(base.split(\"_\")[1].split(\".\")[0])\n","    except Exception:\n","        return None\n","\n","def top_n_live_ids_from_suffix_csv(csv_path: str, n: int) -> List[int]:\n","    df = pd.read_csv(csv_path)\n","    col = \"suffix_number\" if \"suffix_number\" in df.columns else df.columns[-1]\n","    live_ids = (\n","        pd.to_numeric(df[col], errors=\"coerce\")\n","        .dropna()\n","        .astype(int)\n","        .tolist()\n","    )\n","    seen, uniq = set(), []\n","    for lid in live_ids:\n","        if lid not in seen:\n","            seen.add(lid)\n","            uniq.append(lid)\n","        if len(uniq) >= n:\n","            break\n","    return uniq\n","\n","# ====================== MCQ helpers ======================\n","def ensure_two_questions(mcq_obj: Dict[str, Any]) -> Dict[str, Any]:\n","    if isinstance(mcq_obj, dict) and isinstance(mcq_obj.get(\"questions\"), list):\n","        if len(mcq_obj[\"questions\"]) > 2:\n","            mcq_obj[\"questions\"] = mcq_obj[\"questions\"][:2]\n","    return mcq_obj\n","\n","def sanitize_mcq_and_fill_topic(mcq: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n","    # Q1 の正解テキストで Q2 の {TOPIC} を置換\n","    if not isinstance(mcq, dict) or \"questions\" not in mcq:\n","        return None\n","    qs = mcq.get(\"questions\") or []\n","    if len(qs) < 2:\n","        return None\n","    try:\n","        aidx = int(qs[0].get(\"answer_index\", 0))\n","        topic = (qs[0].get(\"choices\") or [])[aidx]\n","    except Exception:\n","        return None\n","    if not isinstance(topic, str) or not topic.strip():\n","        return None\n","    q2q = str(qs[1].get(\"question\", \"\"))\n","    for ph in (\"{TOPIC}\", \"<Q1正解>\", \"{一つ目の問の答え}\"):\n","        q2q = q2q.replace(ph, topic)\n","    qs[1][\"question\"] = q2q\n","    mcq[\"questions\"] = qs[:2]\n","    return mcq\n","\n","def to_public_question(q: Dict[str, Any]) -> Dict[str, Any]:\n","    return {\"question\": str(q.get(\"question\", \"\")),\n","            \"choices\": list(q.get(\"choices\", []))[:4]}\n","\n","def generate_mcq_for_chunk(model: GenerativeModel, combined_text: str, save_label: Optional[str]=None) -> Optional[Dict[str, Any]]:\n","    prompt = prompt_for_mcq_fixed()\n","    content = prompt + \"\\n\" + (combined_text or \"\")\n","    obj = call_gemini_json_with_content(model, content, save_label=save_label)\n","    obj = ensure_two_questions(obj)\n","    obj = sanitize_mcq_and_fill_topic(obj)\n","    try:\n","        qs = obj[\"questions\"]\n","        assert isinstance(qs, list) and len(qs) == 2\n","        for q in qs:\n","            assert isinstance(q.get(\"question\",\"\"), str)\n","            assert isinstance(q.get(\"choices\", []), list) and len(q[\"choices\"]) == 4\n","            assert 0 <= int(q.get(\"answer_index\", 0)) < 4\n","    except Exception:\n","        return None\n","    return obj\n","\n","# ====================== Answering ======================\n","def build_answer_input_text(public_q_json: Dict[str, Any], body_text: str, label: str) -> str:\n","    return f\"\"\"{label}\n","{json.dumps(public_q_json, ensure_ascii=False)}\n","\n","# 本文\n","{body_text or \"\"}\"\"\"\n","\n","def answer_question(model: GenerativeModel, qname: str, source_label: str, public_q: Dict[str, Any],\n","                    body_text: str, save_label: Optional[str]=None) -> Optional[Dict[str, Any]]:\n","    \"\"\"\n","    qname: \"Q1\" or \"Q2\"\n","    source_label: shown in prompt (\"配信ログ本文（チャット＋音声書き起こし）\" or \"チャット本文\")\n","    \"\"\"\n","    prompt = _prompt_for_answering(qname, source_label)\n","    doc = build_answer_input_text(public_q, body_text, f\"# {qname} JSON\")\n","    content = prompt + \"\\n\" + doc\n","    obj = call_gemini_json_with_content(model, content, save_label=save_label)\n","    key = f\"{qname.lower()}_index\"\n","    if isinstance(obj, dict) and \"answers\" in obj and key in (obj[\"answers\"] or {}):\n","        return obj\n","    # Return whatever we got; evaluator will mark invalid-format if needed\n","    return obj if isinstance(obj, dict) else None\n","\n","# ====================== Evaluation ======================\n","def _parse_index_field(ans_obj: Optional[Dict[str, Any]], key: str, num_choices: int = 4) -> Tuple[Optional[int], bool, int]:\n","    \"\"\"\n","    Returns: (pred_index_or_None, is_valid(bool), invalid_format_flag_int)\n","    invalid_format = 1 when the key exists but value is not an int in [0..num_choices-1]\n","    \"\"\"\n","    if not isinstance(ans_obj, dict):\n","        return (None, False, 0)\n","    ans = ans_obj.get(\"answers\", {})\n","    if not isinstance(ans, dict) or key not in ans:\n","        return (None, False, 0)  # no key => not counted as invalid-format\n","    val = ans.get(key)\n","    try:\n","        pred = int(val)\n","    except Exception:\n","        return (None, False, 1)  # present but unparsable\n","    if 0 <= pred < num_choices:\n","        return (pred, True, 0)\n","    else:\n","        return (pred, False, 1)  # present but out of range\n","\n","def evaluate_stage(mcq_full: Dict[str, Any], question_idx: int, ans_obj: Optional[Dict[str, Any]], key: str) -> Dict[str, Any]:\n","    gt = int(mcq_full[\"questions\"][question_idx][\"answer_index\"])\n","    pred, is_valid, invalid_fmt = _parse_index_field(ans_obj, key, num_choices=4)\n","    correct = int(pred == gt) if is_valid else None\n","    return {\"gt\": gt, \"pred\": pred, \"correct\": correct, \"invalid_format\": invalid_fmt}\n","\n","# ====================== Runner (Resume-safe) ======================\n","def process_top_n_lives(\n","    model_gen: GenerativeModel,\n","    model_q1_combined: GenerativeModel,\n","    model_q1_chat_uncond: GenerativeModel,\n","    model_q1_chat_cond: GenerativeModel,\n","    model_q2_combined: GenerativeModel,\n","    model_q2_chat_uncond: GenerativeModel,\n","    model_q2_chat_cond: GenerativeModel,\n","    n: int = TOP_N_LIVE,\n","    min_chars: int = MIN_COMBINED_CHARS,\n",") -> pd.DataFrame:\n","\n","    ensure_local_dirs()\n","    fs = fs_gcs()\n","    lives = top_n_live_ids_from_suffix_csv(SUFFIX_LIST_CSV, n)\n","\n","    # resume: load existing run log if any\n","    existing_rows: List[Dict[str, Any]] = []\n","    if fs.exists(ADOPT_LOG_PATH):\n","        try:\n","            txt = read_gcs_text(ADOPT_LOG_PATH)\n","            if txt.strip():\n","                existing_rows = pd.read_csv(io.StringIO(txt)).to_dict(\"records\")\n","        except Exception:\n","            pass\n","    rows = existing_rows[:]\n","\n","    def flush_checkpoint():\n","        try:\n","            if rows:\n","                write_gcs_text(ADOPT_LOG_PATH, pd.DataFrame(rows).to_csv(index=False))\n","        except Exception:\n","            pass\n","        os.makedirs(LOCAL_RUNLOG_DIR, exist_ok=True)\n","        local_csv = os.path.join(LOCAL_RUNLOG_DIR, f\"adopted_chunks_top{TOP_N_LIVE}.csv\")\n","        try:\n","            pd.DataFrame(rows).to_csv(local_csv, index=False, encoding=\"utf-8\")\n","        except Exception:\n","            pass\n","\n","    ctr = 0\n","\n","    for live_id in tqdm(lives, desc=\"lives\"):\n","        combo_paths = gcs_glob(COMBINED_PREFIX, f\"{live_id}_*.txt\")\n","        if not combo_paths:\n","            rows.append({\"live_id\": live_id, \"chunk_idx\": None, \"status\": \"no_combined\"})\n","            ctr += 1;  flush_checkpoint() if ctr % CHECKPOINT_EVERY == 0 else None\n","            continue\n","\n","        for cp in tqdm(combo_paths, desc=f\"chunks {live_id}\", leave=False):\n","            chunk_idx = parse_chunk_idx_from_path(cp)\n","            chat_path  = f\"{CHATONLY_PREFIX}/{live_id}_{chunk_idx}.txt\"\n","            has_chat = fs.exists(chat_path)\n","\n","            # Read combined\n","            try:\n","                combined_text = read_gcs_text(cp)\n","            except Exception as e:\n","                rows.append({\"live_id\": live_id, \"chunk_idx\": chunk_idx,\n","                             \"status\": \"read_combined_failed\", \"err\": str(e)})\n","                ctr += 1;  flush_checkpoint() if ctr % CHECKPOINT_EVERY == 0 else None\n","                continue\n","            combined_len_raw = len((combined_text or \"\").strip())\n","            if combined_len_raw < min_chars:\n","                rows.append({\"live_id\": live_id, \"chunk_idx\": chunk_idx,\n","                             \"status\": \"skip_short\", \"combined_len_raw\": combined_len_raw})\n","                ctr += 1;  flush_checkpoint() if ctr % CHECKPOINT_EVERY == 0 else None\n","                continue\n","\n","            # Read chat (if exists)\n","            chat_text = \"\"\n","            if has_chat:\n","                try:\n","                    chat_text = read_gcs_text(chat_path)\n","                except Exception as e:\n","                    rows.append({\"live_id\": live_id, \"chunk_idx\": chunk_idx,\n","                                 \"status\": \"read_chat_failed\", \"err\": str(e)})\n","                    ctr += 1;  flush_checkpoint() if ctr % CHECKPOINT_EVERY == 0 else None\n","                    continue\n","\n","            # ---- Paths\n","            q_full_out    = Q_FULL_OUT(live_id, chunk_idx)\n","            q_public_out  = Q_PUB_OUT(live_id, chunk_idx)\n","\n","            a_q1c_out     = A_Q1C_OUT(live_id, chunk_idx)\n","            a_q1chatu_out = A_Q1CHATU_OUT(live_id, chunk_idx)\n","            a_q1chatc_out = A_Q1CHATC_OUT(live_id, chunk_idx)\n","\n","            a_q2c_out     = A_Q2C_OUT(live_id, chunk_idx)\n","            a_q2chatu_out = A_Q2CHATU_OUT(live_id, chunk_idx)\n","            a_q2chatc_out = A_Q2CHATC_OUT(live_id, chunk_idx)\n","\n","            # ---- MCQ (resume-aware)\n","            mcq_full = None\n","            if fs.exists(q_full_out):\n","                try:\n","                    mcq_full = json.loads(read_gcs_text(q_full_out))\n","                except Exception:\n","                    mcq_full = None\n","            if mcq_full is None:\n","                mcq_full = generate_mcq_for_chunk(\n","                    model_gen, combined_text,\n","                    save_label=f\"mcq_gen_combined_live{live_id}_chunk{chunk_idx}\"\n","                )\n","                if not (isinstance(mcq_full, dict) and mcq_full.get(\"questions\") and len(mcq_full[\"questions\"]) == 2):\n","                    rows.append({\"live_id\": live_id, \"chunk_idx\": chunk_idx, \"status\": \"mcq_invalid\"})\n","                    ctr += 1;  flush_checkpoint() if ctr % CHECKPOINT_EVERY == 0 else None\n","                    continue\n","                try:\n","                    write_gcs_text(q_full_out, json.dumps(mcq_full, ensure_ascii=False, indent=2))\n","                except Exception:\n","                    pass\n","                with open(os.path.join(LOCAL_QUESTIONS_FULL_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                    json.dump(mcq_full, f, ensure_ascii=False, indent=2)\n","\n","            # ---- Public (resume-aware)\n","            public_q1 = public_q2 = None\n","            if fs.exists(q_public_out):\n","                try:\n","                    pub = json.loads(read_gcs_text(q_public_out))\n","                    if isinstance(pub, dict) and \"questions\" in pub and len(pub[\"questions\"]) == 2:\n","                        public_q1, public_q2 = pub[\"questions\"][0], pub[\"questions\"][1]\n","                except Exception:\n","                    public_q1 = public_q2 = None\n","            if public_q1 is None or public_q2 is None:\n","                public_q1 = to_public_question(mcq_full[\"questions\"][0])\n","                public_q2 = to_public_question(mcq_full[\"questions\"][1])\n","                try:\n","                    write_gcs_text(q_public_out, json.dumps({\"questions\":[public_q1, public_q2]}, ensure_ascii=False, indent=2))\n","                except Exception:\n","                    pass\n","                with open(os.path.join(LOCAL_QUESTIONS_PUBLIC_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                    json.dump({\"questions\":[public_q1, public_q2]}, f, ensure_ascii=False, indent=2)\n","\n","            # ================= Q1 pipeline =================\n","            # LLM2: Q1 @ Combined\n","            ans_q1_combined = None\n","            if fs.exists(a_q1c_out):\n","                try:\n","                    ans_q1_combined = json.loads(read_gcs_text(a_q1c_out))\n","                except Exception:\n","                    ans_q1_combined = None\n","            if ans_q1_combined is None:\n","                ans_q1_combined = answer_question(\n","                    model_q1_combined, \"Q1\", \"配信ログ本文（チャット＋音声書き起こし）\",\n","                    public_q1, combined_text,\n","                    save_label=f\"answer_Q1_combined_live{live_id}_chunk{chunk_idx}\"\n","                )\n","                if ans_q1_combined is not None:\n","                    try:\n","                        write_gcs_text(a_q1c_out, json.dumps(ans_q1_combined, ensure_ascii=False, indent=2))\n","                    except Exception:\n","                        pass\n","                    with open(os.path.join(LOCAL_ANS_Q1_COMBINED_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(ans_q1_combined, f, ensure_ascii=False, indent=2)\n","\n","            # Evaluate Q1 @ Combined\n","            ev_q1c = evaluate_stage(mcq_full, 0, ans_q1_combined, \"q1_index\")\n","            q1_comb_correct = (ev_q1c[\"correct\"] == 1)\n","\n","            # LLM4: Q1 @ Chat-only (conditional on LLM2 correct + has_chat)\n","            ans_q1_chat_cond = None\n","            ev_q1chat_cond = {\"pred\": None, \"correct\": None, \"invalid_format\": 0}\n","            if q1_comb_correct and has_chat:\n","                if fs.exists(a_q1chatc_out):\n","                    try:\n","                        ans_q1_chat_cond = json.loads(read_gcs_text(a_q1chatc_out))\n","                    except Exception:\n","                        ans_q1_chat_cond = None\n","                if ans_q1_chat_cond is None:\n","                    ans_q1_chat_cond = answer_question(\n","                        model_q1_chat_cond, \"Q1\", \"チャット本文\",\n","                        public_q1, chat_text,\n","                        save_label=f\"answer_Q1_chat_cond_live{live_id}_chunk{chunk_idx}\"\n","                    )\n","                    if ans_q1_chat_cond is not None:\n","                        try:\n","                            write_gcs_text(a_q1chatc_out, json.dumps(ans_q1_chat_cond, ensure_ascii=False, indent=2))\n","                        except Exception:\n","                            pass\n","                        with open(os.path.join(LOCAL_ANS_Q1_CHAT_COND_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                            json.dump(ans_q1_chat_cond, f, ensure_ascii=False, indent=2)\n","                ev_q1chat_cond = evaluate_stage(mcq_full, 0, ans_q1_chat_cond, \"q1_index\")\n","\n","            # LLM3: Q1 @ Chat-only (unconditional)\n","            ans_q1_chat_uncond = None\n","            if fs.exists(a_q1chatu_out):\n","                try:\n","                    ans_q1_chat_uncond = json.loads(read_gcs_text(a_q1chatu_out))\n","                except Exception:\n","                    ans_q1_chat_uncond = None\n","            if ans_q1_chat_uncond is None and has_chat:\n","                ans_q1_chat_uncond = answer_question(\n","                    model_q1_chat_uncond, \"Q1\", \"チャット本文\",\n","                    public_q1, chat_text,\n","                    save_label=f\"answer_Q1_chat_uncond_live{live_id}_chunk{chunk_idx}\"\n","                )\n","                if ans_q1_chat_uncond is not None:\n","                    try:\n","                        write_gcs_text(a_q1chatu_out, json.dumps(ans_q1_chat_uncond, ensure_ascii=False, indent=2))\n","                    except Exception:\n","                        pass\n","                    with open(os.path.join(LOCAL_ANS_Q1_CHAT_UNCOND_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(ans_q1_chat_uncond, f, ensure_ascii=False, indent=2)\n","            ev_q1chat_uncond = evaluate_stage(mcq_full, 0, ans_q1_chat_uncond, \"q1_index\") if has_chat else {\"pred\": None, \"correct\": None, \"invalid_format\": 0}\n","\n","            # ================= Q2 pipeline =================\n","            # LLM5: Q2 @ Combined\n","            ans_q2_combined = None\n","            if fs.exists(a_q2c_out):\n","                try:\n","                    ans_q2_combined = json.loads(read_gcs_text(a_q2c_out))\n","                except Exception:\n","                    ans_q2_combined = None\n","            if ans_q2_combined is None:\n","                ans_q2_combined = answer_question(\n","                    model_q2_combined, \"Q2\", \"配信ログ本文（チャット＋音声書き起こし）\",\n","                    public_q2, combined_text,\n","                    save_label=f\"answer_Q2_combined_live{live_id}_chunk{chunk_idx}\"\n","                )\n","                if ans_q2_combined is not None:\n","                    try:\n","                        write_gcs_text(a_q2c_out, json.dumps(ans_q2_combined, ensure_ascii=False, indent=2))\n","                    except Exception:\n","                        pass\n","                    with open(os.path.join(LOCAL_ANS_Q2_COMBINED_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(ans_q2_combined, f, ensure_ascii=False, indent=2)\n","            ev_q2c = evaluate_stage(mcq_full, 1, ans_q2_combined, \"q2_index\")\n","            q2_comb_correct = (ev_q2c[\"correct\"] == 1)\n","\n","            # LLM7: Q2 @ Chat-only (conditional)\n","            ans_q2_chat_cond = None\n","            ev_q2chat_cond = {\"pred\": None, \"correct\": None, \"invalid_format\": 0}\n","            if q2_comb_correct and has_chat:\n","                if fs.exists(a_q2chatc_out):\n","                    try:\n","                        ans_q2_chat_cond = json.loads(read_gcs_text(a_q2chatc_out))\n","                    except Exception:\n","                        ans_q2_chat_cond = None\n","                if ans_q2_chat_cond is None:\n","                    ans_q2_chat_cond = answer_question(\n","                        model_q2_chat_cond, \"Q2\", \"チャット本文\",\n","                        public_q2, chat_text,\n","                        save_label=f\"answer_Q2_chat_cond_live{live_id}_chunk{chunk_idx}\"\n","                    )\n","                    if ans_q2_chat_cond is not None:\n","                        try:\n","                            write_gcs_text(a_q2chatc_out, json.dumps(ans_q2_chat_cond, ensure_ascii=False, indent=2))\n","                        except Exception:\n","                            pass\n","                        with open(os.path.join(LOCAL_ANS_Q2_CHAT_COND_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                            json.dump(ans_q2_chat_cond, f, ensure_ascii=False, indent=2)\n","                ev_q2chat_cond = evaluate_stage(mcq_full, 1, ans_q2_chat_cond, \"q2_index\")\n","\n","            # LLM6: Q2 @ Chat-only (unconditional)\n","            ans_q2_chat_uncond = None\n","            if fs.exists(a_q2chatu_out):\n","                try:\n","                    ans_q2_chat_uncond = json.loads(read_gcs_text(a_q2chatu_out))\n","                except Exception:\n","                    ans_q2_chat_uncond = None\n","            if ans_q2_chat_uncond is None and has_chat:\n","                ans_q2_chat_uncond = answer_question(\n","                    model_q2_chat_uncond, \"Q2\", \"チャット本文\",\n","                    public_q2, chat_text,\n","                    save_label=f\"answer_Q2_chat_uncond_live{live_id}_chunk{chunk_idx}\"\n","                )\n","                if ans_q2_chat_uncond is not None:\n","                    try:\n","                        write_gcs_text(a_q2chatu_out, json.dumps(ans_q2_chat_uncond, ensure_ascii=False, indent=2))\n","                    except Exception:\n","                        pass\n","                    with open(os.path.join(LOCAL_ANS_Q2_CHAT_UNCOND_DIR, f\"{live_id}_{chunk_idx}.json\"), \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(ans_q2_chat_uncond, f, ensure_ascii=False, indent=2)\n","            ev_q2chat_uncond = evaluate_stage(mcq_full, 1, ans_q2_chat_uncond, \"q2_index\") if has_chat else {\"pred\": None, \"correct\": None, \"invalid_format\": 0}\n","\n","            # ---- Compose row ----\n","            row = {\n","                \"live_id\": live_id,\n","                \"chunk_idx\": chunk_idx,\n","                \"status\": \"ok\",\n","                \"combined_path\": cp,\n","                \"chat_path\": chat_path if has_chat else None,\n","                \"has_chat\": bool(has_chat),\n","\n","                # artifacts\n","                \"mcq_full_path\": q_full_out,\n","                \"mcq_public_path\": q_public_out,\n","\n","                \"ans_q1_combined_path\": a_q1c_out if ans_q1_combined is not None else None,\n","                \"ans_q1_chat_cond_path\": a_q1chatc_out if (q1_comb_correct and has_chat and ans_q1_chat_cond is not None) else None,\n","                \"ans_q1_chat_uncond_path\": a_q1chatu_out if (has_chat and ans_q1_chat_uncond is not None) else None,\n","\n","                \"ans_q2_combined_path\": a_q2c_out if ans_q2_combined is not None else None,\n","                \"ans_q2_chat_cond_path\": a_q2chatc_out if (q2_comb_correct and has_chat and ans_q2_chat_cond is not None) else None,\n","                \"ans_q2_chat_uncond_path\": a_q2chatu_out if (has_chat and ans_q2_chat_uncond is not None) else None,\n","\n","                # eval Q1\n","                \"q1_gt\": ev_q1c[\"gt\"],\n","                \"q1_pred_combined\": ev_q1c[\"pred\"],\n","                \"q1_correct_combined\": ev_q1c[\"correct\"],\n","                \"q1_combined_invalid_format\": ev_q1c[\"invalid_format\"],\n","\n","                \"q1_pred_chat_cond\": ev_q1chat_cond[\"pred\"],\n","                \"q1_chat_cond_correct\": ev_q1chat_cond[\"correct\"],\n","                \"q1_chat_cond_invalid_format\": ev_q1chat_cond[\"invalid_format\"],\n","\n","                \"q1_pred_chat_uncond\": ev_q1chat_uncond[\"pred\"],\n","                \"q1_chat_uncond_correct\": ev_q1chat_uncond[\"correct\"],\n","                \"q1_chat_uncond_invalid_format\": ev_q1chat_uncond[\"invalid_format\"],\n","\n","                # eval Q2\n","                \"q2_gt\": ev_q2c[\"gt\"],\n","                \"q2_pred_combined\": ev_q2c[\"pred\"],\n","                \"q2_correct_combined\": ev_q2c[\"correct\"],\n","                \"q2_combined_invalid_format\": ev_q2c[\"invalid_format\"],\n","\n","                \"q2_pred_chat_cond\": ev_q2chat_cond[\"pred\"],\n","                \"q2_chat_cond_correct\": ev_q2chat_cond[\"correct\"],\n","                \"q2_chat_cond_invalid_format\": ev_q2chat_cond[\"invalid_format\"],\n","\n","                \"q2_pred_chat_uncond\": ev_q2chat_uncond[\"pred\"],\n","                \"q2_chat_uncond_correct\": ev_q2chat_uncond[\"correct\"],\n","                \"q2_chat_uncond_invalid_format\": ev_q2chat_uncond[\"invalid_format\"],\n","            }\n","            rows.append(row)\n","\n","            ctr += 1\n","            if ctr % CHECKPOINT_EVERY == 0:\n","                flush_checkpoint()\n","\n","    flush_checkpoint()\n","    return pd.DataFrame(rows)\n","\n","# ====================== Metrics ======================\n","def compute_metrics(df: pd.DataFrame) -> Dict[str, Any]:\n","    \"\"\"\n","    Return metrics including:\n","      - Combined (Q1/Q2): totals, correct totals, accuracy\n","      - Chat-only UNCONDITIONAL (Q1/Q2): totals, correct totals, accuracy\n","      - Chat-only CONDITIONAL (Q1|LLM2-correct&has_chat / Q2|LLM5-correct&has_chat):\n","          possible_total, trigger_total(=answered_total), answered_total, correct_total, accuracy, not_attempted_total\n","      - invalid_format_total across all stages\n","    \"\"\"\n","    out: Dict[str, Any] = {}\n","\n","    # ---- Q1 Combined\n","    mask_q1c = df[\"q1_correct_combined\"].notna()\n","    out[\"q1_combined_total\"] = int(mask_q1c.sum())\n","    out[\"q1_combined_correct_total\"] = int(df.loc[mask_q1c, \"q1_correct_combined\"].fillna(0).astype(int).sum())\n","    out[\"q1_combined_accuracy\"] = (float(out[\"q1_combined_correct_total\"] / out[\"q1_combined_total\"])\n","                                   if out[\"q1_combined_total\"] else None)\n","\n","    # ---- Q2 Combined\n","    mask_q2c = df[\"q2_correct_combined\"].notna()\n","    out[\"q2_combined_total\"] = int(mask_q2c.sum())\n","    out[\"q2_combined_correct_total\"] = int(df.loc[mask_q2c, \"q2_correct_combined\"].fillna(0).astype(int).sum())\n","    out[\"q2_combined_accuracy\"] = (float(out[\"q2_combined_correct_total\"] / out[\"q2_combined_total\"])\n","                                   if out[\"q2_combined_total\"] else None)\n","\n","    # ---- Q1 Chat-only UNCONDITIONAL (LLM3): only rows with has_chat\n","    mask_q1u = df[\"q1_chat_uncond_correct\"].notna()\n","    out[\"q1_chat_uncond_total\"] = int(mask_q1u.sum())\n","    out[\"q1_chat_uncond_correct_total\"] = int(df.loc[mask_q1u, \"q1_chat_uncond_correct\"].fillna(0).astype(int).sum())\n","    out[\"q1_chat_uncond_accuracy\"] = (float(out[\"q1_chat_uncond_correct_total\"] / out[\"q1_chat_uncond_total\"])\n","                                      if out[\"q1_chat_uncond_total\"] else None)\n","\n","    # ---- Q2 Chat-only UNCONDITIONAL (LLM6): only rows with has_chat\n","    mask_q2u = df[\"q2_chat_uncond_correct\"].notna()\n","    out[\"q2_chat_uncond_total\"] = int(mask_q2u.sum())\n","    out[\"q2_chat_uncond_correct_total\"] = int(df.loc[mask_q2u, \"q2_chat_uncond_correct\"].fillna(0).astype(int).sum())\n","    out[\"q2_chat_uncond_accuracy\"] = (float(out[\"q2_chat_uncond_correct_total\"] / out[\"q2_chat_uncond_total\"])\n","                                      if out[\"q2_chat_uncond_total\"] else None)\n","\n","    # ---- Q1 Chat-only CONDITIONAL (LLM4)\n","    has_chat = df[\"has_chat\"].fillna(False)\n","    possible_q1 = (df[\"q1_correct_combined\"] == 1) & has_chat\n","    out[\"q1_chat_cond_possible_total\"] = int(possible_q1.sum())\n","\n","    attempted_q1 = df[\"ans_q1_chat_cond_path\"].notna() | df[\"q1_chat_cond_correct\"].notna()\n","    trigger_q1 = attempted_q1  # attempted == answered in this pipeline\n","    out[\"q1_chat_cond_trigger_total\"] = int(trigger_q1.sum())\n","    out[\"q1_chat_cond_answered_total\"] = int(trigger_q1.sum())\n","\n","    correct_q1 = df.loc[trigger_q1, \"q1_chat_cond_correct\"].fillna(0).astype(int).sum()\n","    out[\"q1_chat_cond_correct_total\"] = int(correct_q1)\n","    out[\"q1_chat_cond_accuracy\"] = (float(correct_q1 / out[\"q1_chat_cond_answered_total\"])\n","                                    if out[\"q1_chat_cond_answered_total\"] else None)\n","    not_attempted_q1 = possible_q1 & (~trigger_q1)\n","    out[\"q1_chat_cond_not_attempted_total\"] = int(not_attempted_q1.sum())\n","\n","    # ---- Q2 Chat-only CONDITIONAL (LLM7)\n","    possible_q2 = (df[\"q2_correct_combined\"] == 1) & has_chat\n","    out[\"q2_chat_cond_possible_total\"] = int(possible_q2.sum())\n","\n","    attempted_q2 = df[\"ans_q2_chat_cond_path\"].notna() | df[\"q2_chat_cond_correct\"].notna()\n","    trigger_q2 = attempted_q2\n","    out[\"q2_chat_cond_trigger_total\"] = int(trigger_q2.sum())\n","    out[\"q2_chat_cond_answered_total\"] = int(trigger_q2.sum())\n","\n","    correct_q2 = df.loc[trigger_q2, \"q2_chat_cond_correct\"].fillna(0).astype(int).sum()\n","    out[\"q2_chat_cond_correct_total\"] = int(correct_q2)\n","    out[\"q2_chat_cond_accuracy\"] = (float(correct_q2 / out[\"q2_chat_cond_answered_total\"])\n","                                    if out[\"q2_chat_cond_answered_total\"] else None)\n","    not_attempted_q2 = possible_q2 & (~trigger_q2)\n","    out[\"q2_chat_cond_not_attempted_total\"] = int(not_attempted_q2.sum())\n","\n","    # ---- invalid-format counts across all answer stages\n","    invalid_cols = [\n","        \"q1_combined_invalid_format\", \"q1_chat_uncond_invalid_format\", \"q1_chat_cond_invalid_format\",\n","        \"q2_combined_invalid_format\", \"q2_chat_uncond_invalid_format\", \"q2_chat_cond_invalid_format\",\n","    ]\n","    total_invalid = 0\n","    for c in invalid_cols:\n","        if c in df.columns:\n","            total_invalid += int(df[c].fillna(0).astype(int).sum())\n","    out[\"invalid_format_total\"] = int(total_invalid)\n","\n","    return out\n","\n","def save_metrics(metrics: Dict[str, Any]) -> None:\n","    try:\n","        write_gcs_text(METRICS_JSON_PATH, json.dumps(metrics, ensure_ascii=False, indent=2))\n","    except Exception:\n","        pass\n","    os.makedirs(LOCAL_RUNLOG_DIR, exist_ok=True)\n","    local_metrics = os.path.join(LOCAL_RUNLOG_DIR, \"metrics.json\")\n","    try:\n","        with open(local_metrics, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(metrics, f, ensure_ascii=False, indent=2)\n","    except Exception:\n","        pass\n","\n","# ====================== Examples (3 correct / 3 incorrect) ======================\n","def _load_public_q(q_public_path: str) -> Optional[Dict[str, Any]]:\n","    try:\n","        data = json.loads(read_gcs_text(q_public_path))\n","        if isinstance(data, dict) and \"questions\" in data and len(data[\"questions\"]) == 2:\n","            return data\n","    except Exception:\n","        # try local\n","        try:\n","            with open(os.path.join(LOCAL_QUESTIONS_PUBLIC_DIR, os.path.basename(q_public_path)), \"r\", encoding=\"utf-8\") as f:\n","                data = json.load(f)\n","                if isinstance(data, dict) and \"questions\" in data and len(data[\"questions\"]) == 2:\n","                    return data\n","        except Exception:\n","            pass\n","    return None\n","\n","def _load_combined_text(path: Optional[str]) -> Optional[str]:\n","    if not isinstance(path, str):\n","        return None\n","    try:\n","        return read_gcs_text(path)\n","    except Exception:\n","        return None\n","\n","def gather_examples(df: pd.DataFrame, stage: str, correct_flag: int, k: int = 3) -> List[Dict[str, Any]]:\n","    \"\"\"\n","    stage: one of\n","      - 'q1_combined', 'q1_chat_uncond', 'q1_chat_cond',\n","      - 'q2_combined', 'q2_chat_uncond', 'q2_chat_cond'\n","    correct_flag: 1 -> correct examples, 0 -> incorrect examples\n","    ★ combined_text (10分の chat+audio) を例に含める\n","    \"\"\"\n","    # column mapping\n","    pred_col_map = {\n","        \"q1_combined\": \"q1_pred_combined\",\n","        \"q1_chat_uncond\": \"q1_pred_chat_uncond\",\n","        \"q1_chat_cond\": \"q1_pred_chat_cond\",\n","        \"q2_combined\": \"q2_pred_combined\",\n","        \"q2_chat_uncond\": \"q2_pred_chat_uncond\",\n","        \"q2_chat_cond\": \"q2_pred_chat_cond\",\n","    }\n","    corr_col_map = {\n","        \"q1_combined\": \"q1_correct_combined\",\n","        \"q1_chat_uncond\": \"q1_chat_uncond_correct\",\n","        \"q1_chat_cond\": \"q1_chat_cond_correct\",\n","        \"q2_combined\": \"q2_correct_combined\",\n","        \"q2_chat_uncond\": \"q2_chat_uncond_correct\",\n","        \"q2_chat_cond\": \"q2_chat_cond_correct\",\n","    }\n","    qidx_map = {\n","        \"q1_combined\": 0, \"q1_chat_uncond\": 0, \"q1_chat_cond\": 0,\n","        \"q2_combined\": 1, \"q2_chat_uncond\": 1, \"q2_chat_cond\": 1,\n","    }\n","\n","    pred_col = pred_col_map[stage]\n","    corr_col = corr_col_map[stage]\n","    qidx = qidx_map[stage]\n","\n","    subset = df[df[corr_col] == correct_flag].copy()\n","    if subset.empty:\n","        return []\n","    examples = []\n","    for _, r in subset.head(k).iterrows():\n","        pub_path = r.get(\"mcq_public_path\")\n","        pq = _load_public_q(pub_path) if isinstance(pub_path, str) else None\n","        if not pq:\n","            continue\n","        qobj = pq[\"questions\"][qidx]\n","        pred = r.get(pred_col)\n","\n","        combined_path = r.get(\"combined_path\")\n","        combined_text = _load_combined_text(combined_path)\n","\n","        ex = {\n","            \"stage\": stage,\n","            \"live_id\": int(r[\"live_id\"]) if pd.notna(r[\"live_id\"]) else None,\n","            \"chunk_idx\": int(r[\"chunk_idx\"]) if pd.notna(r[\"chunk_idx\"]) else None,\n","            \"combined_path\": combined_path,\n","            \"combined_text\": combined_text,  # ★ 10分の chat+audio テキストを格納\n","            \"question\": qobj.get(\"question\"),\n","            \"choices\": qobj.get(\"choices\"),\n","            \"pred_index\": int(pred) if pd.notna(pred) else None,\n","            \"pred_choice_text\": (qobj.get(\"choices\")[int(pred)] if pd.notna(pred) and 0 <= int(pred) < 4 else None),\n","        }\n","        examples.append(ex)\n","    return examples\n","\n","def save_examples(examples: Dict[str, List[Dict[str, Any]]]) -> None:\n","    # Local\n","    os.makedirs(LOCAL_EXAMPLES_DIR, exist_ok=True)\n","    local_path = os.path.join(LOCAL_EXAMPLES_DIR, \"examples.json\")\n","    with open(local_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(examples, f, ensure_ascii=False, indent=2)\n","    # GCS\n","    try:\n","        write_gcs_text(f\"{EXAMPLES_PREFIX}/examples.json\", json.dumps(examples, ensure_ascii=False, indent=2))\n","    except Exception:\n","        pass\n","\n","# ====================== Main ======================\n","if __name__ == \"__main__\":\n","    # init models (all gemini-2.5-pro)\n","    model_gen             = init_vertex_ai(MODEL_NAME_QGEN)            # LLM1\n","    model_q1_combined     = init_vertex_ai(MODEL_NAME_Q1_COMBINED)     # LLM2\n","    model_q1_chat_uncond  = init_vertex_ai(MODEL_NAME_Q1_CHAT_UNCOND)  # LLM3\n","    model_q1_chat_cond    = init_vertex_ai(MODEL_NAME_Q1_CHAT_COND)    # LLM4\n","    model_q2_combined     = init_vertex_ai(MODEL_NAME_Q2_COMBINED)     # LLM5\n","    model_q2_chat_uncond  = init_vertex_ai(MODEL_NAME_Q2_CHAT_UNCOND)  # LLM6\n","    model_q2_chat_cond    = init_vertex_ai(MODEL_NAME_Q2_CHAT_COND)    # LLM7\n","\n","    df = process_top_n_lives(\n","        model_gen,\n","        model_q1_combined, model_q1_chat_uncond, model_q1_chat_cond,\n","        model_q2_combined, model_q2_chat_uncond, model_q2_chat_cond,\n","        n=TOP_N_LIVE\n","    )\n","\n","    metrics = compute_metrics(df)\n","    print(json.dumps(metrics, ensure_ascii=False, indent=2))\n","    save_metrics(metrics)\n","\n","    # ---- Gather & save 3 correct / 3 incorrect examples (Q1/Q2 Combined & Unconditional Chat) ----\n","    examples = {\n","        \"q1_combined_correct_top3\":      gather_examples(df, \"q1_combined\", correct_flag=1, k=3),\n","        \"q1_combined_incorrect_top3\":    gather_examples(df, \"q1_combined\", correct_flag=0, k=3),\n","        \"q2_combined_correct_top3\":      gather_examples(df, \"q2_combined\", correct_flag=1, k=3),\n","        \"q2_combined_incorrect_top3\":    gather_examples(df, \"q2_combined\", correct_flag=0, k=3),\n","\n","        \"q1_chat_uncond_correct_top3\":   gather_examples(df, \"q1_chat_uncond\", correct_flag=1, k=3),\n","        \"q1_chat_uncond_incorrect_top3\": gather_examples(df, \"q1_chat_uncond\", correct_flag=0, k=3),\n","        \"q2_chat_uncond_correct_top3\":   gather_examples(df, \"q2_chat_uncond\", correct_flag=1, k=3),\n","        \"q2_chat_uncond_incorrect_top3\": gather_examples(df, \"q2_chat_uncond\", correct_flag=0, k=3),\n","\n","        # （必要なら条件付きチャット側も保存）\n","        \"q1_chat_cond_correct_top3\":     gather_examples(df, \"q1_chat_cond\", correct_flag=1, k=3),\n","        \"q1_chat_cond_incorrect_top3\":   gather_examples(df, \"q1_chat_cond\", correct_flag=0, k=3),\n","        \"q2_chat_cond_correct_top3\":     gather_examples(df, \"q2_chat_cond\", correct_flag=1, k=3),\n","        \"q2_chat_cond_incorrect_top3\":   gather_examples(df, \"q2_chat_cond\", correct_flag=0, k=3),\n","    }\n","    save_examples(examples)\n","\n","    # ---- Also print invalid-format total for quick visibility ----\n","    print(\"invalid_format_total:\", metrics.get(\"invalid_format_total\"))\n"]}]}