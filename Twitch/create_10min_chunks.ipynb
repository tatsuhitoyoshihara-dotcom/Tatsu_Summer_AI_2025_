{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPibIcl47Dq7MLTdGtqpdd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZKBOi7Oyw_i"},"outputs":[],"source":["# ============================================================\n","# Reset & Rebuild: Twitch -> 10min Chunks (CHAT+SPEECH / CHAT-only)\n","# - æ—¢å­˜æˆæœç‰©(å‡ºåŠ›)ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ã€ã‚¼ãƒ­ã‹ã‚‰å†ä½œæˆ\n","# - å…¥åŠ›(raw/chat, processed/whisper_tiny_csv)ã¯èª­ã¿è¾¼ã¿å°‚ç”¨\n","# ============================================================\n","\n","from __future__ import annotations\n","import os, io, re, json, math\n","from typing import List, Dict, Optional, Iterable, Tuple\n","\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from google.cloud import storage\n","import gcsfs\n","\n","# -----------------------\n","# è¨­å®š\n","# -----------------------\n","GCS_BUCKET      = \"dena-ai-intern-yoshihara-data\"\n","GCS_ROOT_PREFIX = \"twitch_v2\"\n","RUN_ID          = \"20250910_190657\"     # å¤±æ•—ã—ãŸ run_id ã«åˆã‚ã›ã‚‹\n","CHUNK_SECONDS   = 10 * 60               # 10åˆ†\n","\n","# â˜… æˆæœç‰©ã®ä¿å­˜å…ˆï¼ˆPocochaã¨åŒã˜å‘½åï¼‰\n","OUT_COMBINED_PREFIX = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_twitch_comment_speeech_combined\"\n","OUT_CHATONLY_PREFIX = \"gs://dena-ai-intern-yoshihara-data/yoshi_LLMQA_twitch_comment_only\"\n","OUT_EXT             = \".txt\"\n","\n","# ï¼ˆä»»æ„ï¼‰å‡¦ç†å¯¾è±¡ VOD ã‚’é™å®šã™ã‚‹å ´åˆã¯åˆ—æŒ™\n","VOD_WHITELIST: Optional[List[str]] = None   # ä¾‹: [\"2561964528\"]\n","\n","# å‰Šé™¤ã‚’æœ¬å½“ã«å®Ÿè¡Œã™ã‚‹ã‹ï¼ˆTrue ã§å®Ÿè¡Œï¼‰\n","PURGE_OUTPUTS = True\n","\n","# -----------------------\n","# GCS ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n","# -----------------------\n","gcs_client = storage.Client()\n","bucket = gcs_client.bucket(GCS_BUCKET)\n","fs = gcsfs.GCSFileSystem()\n","\n","def parse_gcs_path(gcs_path: str) -> Tuple[str, str]:\n","    assert gcs_path.startswith(\"gs://\")\n","    rest = gcs_path[5:]\n","    bkt, _, prefix = rest.partition(\"/\")\n","    prefix = prefix.rstrip(\"/\") + (\"/\" if prefix and not prefix.endswith(\"/\") else \"\")\n","    return bkt, prefix\n","\n","def blob_exists(key: str) -> bool:\n","    return bucket.blob(key).exists()\n","\n","def download_bytes(key: str) -> bytes:\n","    bl = bucket.blob(key)\n","    if not bl.exists():\n","        raise FileNotFoundError(f\"not found: gs://{GCS_BUCKET}/{key}\")\n","    return bl.download_as_bytes()\n","\n","def download_csv_df(key: str) -> pd.DataFrame:\n","    data = download_bytes(key)\n","    return pd.read_csv(io.BytesIO(data))\n","\n","def whisper_csv_key(vod_id: str) -> str:\n","    return f\"{GCS_ROOT_PREFIX}/{RUN_ID}/processed/whisper_tiny_csv/{vod_id}.csv\"\n","\n","def chat_json_key(vod_id: str) -> str:\n","    return f\"{GCS_ROOT_PREFIX}/{RUN_ID}/raw/chat/{vod_id}_chat.json\"\n","\n","def list_whisper_csv_vods(run_id: str) -> List[str]:\n","    prefix = f\"{GCS_ROOT_PREFIX}/{run_id}/processed/whisper_tiny_csv/\"\n","    vods = []\n","    for bl in gcs_client.list_blobs(GCS_BUCKET, prefix=prefix):\n","        base = os.path.basename(bl.name)\n","        if base.endswith(\".csv\") and (bl.size or 0) > 0:\n","            vods.append(base[:-4])\n","    return sorted(set(vods))\n","\n","# -----------------------\n","# æ™‚åˆ»ãƒ»ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢\n","# -----------------------\n","_HMS_RE = re.compile(r\"^(?P<h>\\d{1,2}):(?P<m>\\d{2}):(?P<s>\\d{2})(?:\\.(?P<ms>\\d{1,3}))?$\")\n","\n","def hms_to_seconds(hms: str) -> int:\n","    if not isinstance(hms, str):\n","        return 0\n","    m = _HMS_RE.match(hms.strip())\n","    if not m:\n","        return 0\n","    h = int(m.group(\"h\")); mi = int(m.group(\"m\")); s = int(m.group(\"s\"))\n","    ms = int(m.group(\"ms\") or 0)\n","    return int(h*3600 + mi*60 + s + (ms/1000.0))\n","\n","def to_int_seconds_safe(x) -> int:\n","    try:\n","        v = float(x)\n","        if math.isnan(v):\n","            return 0\n","        return max(0, int(v))\n","    except Exception:\n","        return 0\n","\n","def extract_chat_text(m: Dict) -> str:\n","    \"\"\"Twitchãƒãƒ£ãƒƒãƒˆæœ¬æ–‡æŠ½å‡ºï¼ˆfragments â†’ body/text â†’ top-level text/body/contentï¼‰\"\"\"\n","    msg = m.get(\"message\")\n","    text = \"\"\n","    if isinstance(msg, dict):\n","        frags = msg.get(\"fragments\")\n","        if isinstance(frags, list):\n","            text = \"\".join(str(f.get(\"text\") or \"\") for f in frags)\n","        else:\n","            text = (msg.get(\"body\") or msg.get(\"text\") or \"\")\n","    elif isinstance(msg, str):\n","        text = msg\n","    if not text:\n","        text = (m.get(\"text\") or m.get(\"body\") or m.get(\"content\") or \"\")\n","        if not text and isinstance(m.get(\"fragments\"), list):\n","            text = \"\".join(str(f.get(\"text\") or \"\") for f in m[\"fragments\"])\n","    return str(text).strip()\n","\n","# -----------------------\n","# ãƒ­ãƒ¼ãƒ€\n","# -----------------------\n","def load_chat_df(vod_id: str) -> pd.DataFrame:\n","    raw = download_bytes(chat_json_key(vod_id))\n","    try:\n","        obj = json.loads(raw.decode(\"utf-8\"))\n","    except Exception:\n","        obj = json.loads(raw.decode(\"utf-8\", errors=\"ignore\"))\n","\n","    if isinstance(obj, dict):\n","        if \"messages\" in obj and isinstance(obj[\"messages\"], list):\n","            messages = obj[\"messages\"]\n","        elif \"data\" in obj and isinstance(obj[\"data\"], list):\n","            messages = obj[\"data\"]\n","        else:\n","            messages = None\n","            for v in obj.values():\n","                if isinstance(v, list):\n","                    messages = v; break\n","            if messages is None:\n","                raise ValueError(\"CHAT JSON ã®ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n","    elif isinstance(obj, list):\n","        messages = obj\n","    else:\n","        raise ValueError(\"CHAT JSON ã®å½¢å¼ãŒä¸æ­£ã§ã™\")\n","\n","    rows = []\n","    for i, m in enumerate(messages):\n","        if not isinstance(m, dict):\n","            continue\n","        text = extract_chat_text(m)\n","        if not text:\n","            continue\n","        t_candidates = [m.get(\"offset_seconds\"), m.get(\"offsetSecs\"),\n","                        m.get(\"offset\"), m.get(\"elapsed_seconds\"), m.get(\"t\")]\n","        t_val = None\n","        for cand in t_candidates:\n","            if cand is None:\n","                continue\n","            t_val = to_int_seconds_safe(cand); break\n","        if (t_val is None or t_val == 0):\n","            rel = m.get(\"relative_time\") or m.get(\"time\") or m.get(\"display_time\")\n","            if isinstance(rel, str) and \":\" in rel:\n","                t_val = hms_to_seconds(rel)\n","        if t_val is None:\n","            t_val = 0\n","        rows.append({\"source\":\"CHAT\",\"t\":t_val,\"content\":text,\"orig_idx\":i})\n","\n","    df = pd.DataFrame(rows, columns=[\"source\",\"t\",\"content\",\"orig_idx\"])\n","    if df.empty:\n","        return pd.DataFrame(columns=[\"source\",\"t\",\"content\",\"orig_idx\"])\n","    return df.sort_values([\"t\",\"orig_idx\"], kind=\"mergesort\").reset_index(drop=True)\n","\n","def load_speech_df(vod_id: str) -> pd.DataFrame:\n","    df = download_csv_df(whisper_csv_key(vod_id))\n","    need = {\"timestamp_start\",\"transcription\"}\n","    missing = [c for c in need if c not in df.columns]\n","    if missing:\n","        raise ValueError(f\"Whisper CSV åˆ—ä¸è¶³: {missing}\")\n","    rows = []\n","    for i, r in enumerate(df.itertuples(index=False)):\n","        text = str(getattr(r, \"transcription\", \"\")).strip()\n","        if not text:\n","            continue\n","        t = hms_to_seconds(str(getattr(r, \"timestamp_start\", \"0:00:00\")))\n","        rows.append({\"source\":\"SPEECH\",\"t\":t,\"content\":text,\"orig_idx\":i})\n","    out = pd.DataFrame(rows, columns=[\"source\",\"t\",\"content\",\"orig_idx\"])\n","    if out.empty:\n","        return pd.DataFrame(columns=[\"source\",\"t\",\"content\",\"orig_idx\"])\n","    return out.sort_values([\"t\",\"orig_idx\"], kind=\"mergesort\").reset_index(drop=True)\n","\n","# -----------------------\n","# ãƒãƒ£ãƒ³ã‚¯ & æ›¸ãå‡ºã—\n","# -----------------------\n","def ensure_prefix_no_trailing_slash(prefix: str) -> str:\n","    return prefix[:-1] if prefix.endswith(\"/\") else prefix\n","\n","def out_path(prefix: str, vod_id: str, chunk_idx: int, ext: str = OUT_EXT) -> str:\n","    prefix = ensure_prefix_no_trailing_slash(prefix)\n","    return f\"{prefix}/{vod_id}_{chunk_idx}{ext}\"\n","\n","def build_lines(df: pd.DataFrame) -> List[str]:\n","    if df.empty:\n","        return []\n","    lines = []\n","    for row in df.itertuples(index=False):\n","        text_clean = str(row.content).replace(\"\\n\", \" \").strip()\n","        lines.append(f\"[{row.source}] {text_clean}\")\n","    return lines\n","\n","def iter_chunk_indices(max_t: int, chunk_seconds: int = CHUNK_SECONDS) -> Iterable[int]:\n","    if max_t < 0:\n","        return []\n","    last_chunk = (max_t // chunk_seconds) + 1\n","    return range(1, last_chunk + 1)\n","\n","def export_vod(vod_id: str) -> Tuple[int,int]:\n","    \"\"\"1 VOD ã‚’å‡¦ç†ã—ã¦ GCS ã«å‡ºåŠ›ã€‚æˆ»ã‚Šå€¤: (created, skipped)\"\"\"\n","    created = 0; skipped = 0\n","\n","    if not blob_exists(chat_json_key(vod_id)):\n","        tqdm.write(f\"âš ï¸ chat ãªã—: {vod_id}\"); return created, skipped\n","    if not blob_exists(whisper_csv_key(vod_id)):\n","        tqdm.write(f\"âš ï¸ whisper CSV ãªã—: {vod_id}\"); return created, skipped\n","\n","    chat_df   = load_chat_df(vod_id)\n","    speech_df = load_speech_df(vod_id)\n","\n","    chat_df   = chat_df[chat_df[\"content\"].astype(str).str.len() > 0]\n","    speech_df = speech_df[speech_df[\"content\"].astype(str).str.len() > 0]\n","\n","    merged = pd.concat([chat_df, speech_df], ignore_index=True)\n","    source_order = merged[\"source\"].map({\"CHAT\":0, \"SPEECH\":1}).fillna(2).astype(int)\n","    merged = merged.assign(_o=source_order).sort_values([\"t\",\"_o\",\"orig_idx\"], kind=\"mergesort\").drop(columns=[\"_o\"])\n","\n","    if not merged.empty:\n","        merged[\"chunk_idx\"] = (merged[\"t\"] // CHUNK_SECONDS) + 1\n","    if not chat_df.empty:\n","        chat_df = chat_df.sort_values([\"t\",\"orig_idx\"], kind=\"mergesort\").reset_index(drop=True)\n","        chat_df[\"chunk_idx\"] = (chat_df[\"t\"] // CHUNK_SECONDS) + 1\n","\n","    max_t_candidates = []\n","    if not merged.empty:\n","        max_t_candidates.append(int(merged[\"t\"].max()))\n","    if not chat_df.empty:\n","        max_t_candidates.append(int(chat_df[\"t\"].max()))\n","    max_t = max(max_t_candidates) if max_t_candidates else -1\n","    if max_t < 0:\n","        tqdm.write(f\"â„¹ï¸ æœ‰åŠ¹ãƒ†ã‚­ã‚¹ãƒˆãªã—: {vod_id}\")\n","        return created, skipped\n","\n","    for idx in iter_chunk_indices(max_t, CHUNK_SECONDS):\n","        # Combined\n","        out_combined = out_path(OUT_COMBINED_PREFIX, vod_id, idx, OUT_EXT)\n","        if fs.exists(out_combined):\n","            skipped += 1\n","        else:\n","            part = merged[merged[\"chunk_idx\"] == idx]\n","            lines = build_lines(part)\n","            if lines:\n","                with fs.open(out_combined, \"w\", encoding=\"utf-8\") as f:\n","                    txt = \"\\n\".join(lines)\n","                    f.write(txt if txt.endswith(\"\\n\") else txt + \"\\n\")\n","                created += 1\n","\n","        # Chat-only\n","        out_chatonly = out_path(OUT_CHATONLY_PREFIX, vod_id, idx, OUT_EXT)\n","        if fs.exists(out_chatonly):\n","            skipped += 1\n","        else:\n","            part_c = chat_df[chat_df[\"chunk_idx\"] == idx]\n","            lines_c = build_lines(part_c)\n","            if lines_c:\n","                with fs.open(out_chatonly, \"w\", encoding=\"utf-8\") as f:\n","                    txtc = \"\\n\".join(lines_c)\n","                    f.write(txtc if txtc.endswith(\"\\n\") else txtc + \"\\n\")\n","                created += 1\n","\n","    return created, skipped\n","\n","# -----------------------\n","# æˆæœç‰©ã®å…¨å‰Šé™¤ï¼ˆPurgeï¼‰\n","# -----------------------\n","def purge_outputs(out_prefix_gs: str, vod_whitelist: Optional[List[str]] = None) -> int:\n","    \"\"\"\n","    æŒ‡å®šã® 'gs://bucket/prefix' é…ä¸‹ã® .txt æˆæœç‰©ã‚’å‰Šé™¤ã€‚\n","    vod_whitelist ãŒã‚ã‚Œã° '<prefix>/<vod_id>_' ã®å‰æ–¹ä¸€è‡´ã ã‘å‰Šé™¤ã€‚\n","    æˆ»ã‚Šå€¤: å‰Šé™¤ä»¶æ•°\n","    \"\"\"\n","    bkt, pref = parse_gcs_path(out_prefix_gs)\n","    assert bkt == GCS_BUCKET, f\"æƒ³å®šå¤–ã®ãƒã‚±ãƒƒãƒˆ: {bkt}\"\n","\n","    to_delete = []\n","    if vod_whitelist:\n","        for vid in set(vod_whitelist):\n","            subprefix = pref + f\"{vid}_\"\n","            for bl in gcs_client.list_blobs(bkt, prefix=subprefix):\n","                if bl.name.endswith(\".txt\"):\n","                    to_delete.append(bl.name)\n","    else:\n","        for bl in gcs_client.list_blobs(bkt, prefix=pref):\n","            if bl.name.endswith(\".txt\"):\n","                to_delete.append(bl.name)\n","\n","    if not to_delete:\n","        print(f\"ğŸ§¹ å‰Šé™¤å¯¾è±¡ãªã—: {out_prefix_gs}\")\n","        return 0\n","\n","    print(f\"ğŸ§¹ å‰Šé™¤å¯¾è±¡ {len(to_delete)} ä»¶: {out_prefix_gs}\")\n","    for name in tqdm(to_delete, desc=f\"delete {os.path.basename(out_prefix_gs)}\", unit=\"file\"):\n","        try:\n","            bucket.blob(name).delete()\n","        except Exception as e:\n","            print(f\"  âš ï¸ deleteå¤±æ•—: gs://{GCS_BUCKET}/{name} ({e})\")\n","    return len(to_delete)\n","\n","# -----------------------\n","# ãƒ¡ã‚¤ãƒ³\n","# -----------------------\n","def main():\n","    # 1) å¯¾è±¡VODã®æŠ½å‡ºï¼ˆWhisper CSV ã‚ã‚Šï¼‰\n","    vods = list_whisper_csv_vods(RUN_ID)\n","    if VOD_WHITELIST:\n","        vods = [v for v in vods if v in set(VOD_WHITELIST)]\n","\n","    # chat ã‚‚ã‚ã‚‹ã‚‚ã®ã ã‘\n","    vods_ok = []\n","    for vid in vods:\n","        if blob_exists(chat_json_key(vid)) and blob_exists(whisper_csv_key(vid)):\n","            vods_ok.append(vid)\n","\n","    if not vods_ok:\n","        print(\"å‡¦ç†å¯¾è±¡ VOD ãªã—ï¼ˆchat ã¨ whisper CSV ã®ä¸¡æ–¹ãŒå¿…è¦ï¼‰\")\n","        return\n","\n","    # 2) æˆæœç‰©ã®å‰Šé™¤ï¼ˆè¦æ±‚é€šã‚Šã€Œæœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™ã€ãŸã‚ï¼‰\n","    if PURGE_OUTPUTS:\n","        print(\"=== æˆæœç‰©ã®å…¨å‰Šé™¤ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆå…¥åŠ›ã¯å¤‰æ›´ã—ã¾ã›ã‚“ï¼‰===\")\n","        n1 = purge_outputs(OUT_COMBINED_PREFIX, vods_ok if VOD_WHITELIST else None)\n","        n2 = purge_outputs(OUT_CHATONLY_PREFIX, vods_ok if VOD_WHITELIST else None)\n","        print(f\"å‰Šé™¤ã‚µãƒãƒª: combined={n1}, chatonly={n2}\")\n","\n","    # 3) å†ä½œæˆï¼ˆã‚¼ãƒ­ã‹ã‚‰ï¼‰\n","    total_created = 0; total_skipped = 0\n","    pbar = tqdm(vods_ok, desc=\"rebuild VODs\", unit=\"vod\")\n","    for vid in pbar:\n","        c, s = export_vod(vid)\n","        total_created += c; total_skipped += s\n","        pbar.set_postfix_str(f\"created={total_created} skipped={total_skipped}\")\n","\n","    print(\"\\n=== å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ ===\")\n","    print(f\"ä½œæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_created}\")\n","    print(f\"ã‚¹ã‚­ãƒƒãƒ—æ•°    : {total_skipped}ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ã«ã‚ˆã‚Šï¼‰\")\n","    print(f\"å‡ºåŠ›å…ˆ Combined: {OUT_COMBINED_PREFIX}\")\n","    print(f\"å‡ºåŠ›å…ˆ ChatOnly: {OUT_CHATONLY_PREFIX}\")\n","\n","# å®Ÿè¡Œ\n","main()\n"]}]}